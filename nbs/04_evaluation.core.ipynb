{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# default_exp evaluation.core"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "\n",
    "> API details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# export\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from icodegen.data.core import replace_spec_toks_to_original, java_special_tokens\n",
    "from icodegen.data.transforms import (\n",
    "    code_token_randomizer,\n",
    "    line_randomizer,\n",
    "    java_comment_remover,\n",
    "    transform_df,\n",
    ")\n",
    "from icodegen.model.core import Model, RNNModel, TransformerHFModel, WildModel, _loss\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Optional"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# hide\n",
    "# Setting up testing data\n",
    "from transformers import GPT2TokenizerFast, TFGPT2LMHeadModel\n",
    "from icodegen.model.core import RNNModel\n",
    "\n",
    "# Using tiny-gpt2 for just quick tests since it is... tiny :)\n",
    "trnsfr_tokenizer = GPT2TokenizerFast.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "tokenizer = trnsfr_tokenizer.backend_tokenizer\n",
    "trnsfr = TFGPT2LMHeadModel.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "# trnsfr_model = TransformerModel(tokenizer, trnsfr)\n",
    "\n",
    "rnn_type = \"gru\"\n",
    "n_layers = 1\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 128\n",
    "rnn_units = 128\n",
    "batch_size = 1\n",
    "out_path = \"/tmp\"\n",
    "gru_model = RNNModel(\n",
    "    rnn_type,\n",
    "    n_layers,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    rnn_units,\n",
    "    batch_size,\n",
    "    out_path,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "df_fake = pd.DataFrame(\n",
    "    [\"aaaa(bb(aaaa(bb()()ccc)dd)()ccc)dd\", \"aaaa(bb()ccccc)dd\"], columns=[\"code\"]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898669.0, style=ProgressStyle(descripti…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1fd0c100c7047f88e5cc10df1c3191d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb2ccb8f76964ba5a8202899b2827e3d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=90.0, style=ProgressStyle(description_w…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb4dc703243841a79333e66097e017f5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=26.0, style=ProgressStyle(description_w…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7059308bc154cab928e3a3f50f6f1a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=662.0, style=ProgressStyle(description_…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8839ba00c3814b7fb464e7da0dcc8f0c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=452120.0, style=ProgressStyle(descripti…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a41599ac3af43e8a31c8a35e1181540"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at sshleifer/tiny-gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# export\n",
    "def get_mean_probs(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean probability of each token that the model\n",
    "    should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a numpy array of the mean probability for each token in the model's vocab\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    counts = [0] * model.tokenizer.get_vocab_size()\n",
    "    sum_probs = [0.0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            counts[idx] += 1\n",
    "            sum_probs[idx] += p[idx]\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    counts = np.array(counts)\n",
    "    sum_probs = np.array(sum_probs)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(counts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_probs = np.divide(sum_probs, counts, out=nans, where=counts != 0)\n",
    "    # TODO: convert to dictionary with keys as tokens\n",
    "    mean_probs = {\n",
    "        model.tokenizer.id_to_token(i): mean_probs[i] for i in range(len(mean_probs))\n",
    "    }\n",
    "    return mean_probs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NON_NAN_PROBS_MEAN = np.array(\n",
    "    [\n",
    "        2.01237513e-05,\n",
    "        1.98944481e-05,\n",
    "        2.01449202e-05,\n",
    "        2.04353437e-05,\n",
    "        2.02043060e-05,\n",
    "        2.02826177e-05,\n",
    "        2.09888076e-05,\n",
    "        2.07051467e-05,\n",
    "        1.98100976e-05,\n",
    "        2.02152678e-05,\n",
    "        2.02035244e-05,\n",
    "        2.10283021e-05,\n",
    "    ]\n",
    ")\n",
    "\n",
    "mean_probs = np.array(list(get_mean_probs(df_fake, trnsfr_model).values()))\n",
    "non_nan_idx = np.argwhere(~np.isnan(mean_probs)).flatten()\n",
    "non_nan_mean_prob = mean_probs[non_nan_idx]\n",
    "\n",
    "assert np.isclose(non_nan_mean_prob, NON_NAN_PROBS_MEAN, atol=1.0e-6).all()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NON_NAN_PROBS_MEAN = np.array(\n",
    "    [\n",
    "        1.99270412e-05,\n",
    "        1.99168703e-05,\n",
    "        1.98815596e-05,\n",
    "        1.99057849e-05,\n",
    "        1.98800869e-05,\n",
    "        1.98893995e-05,\n",
    "        1.98797388e-05,\n",
    "        1.98960342e-05,\n",
    "        1.99086674e-05,\n",
    "        1.98605580e-05,\n",
    "        1.98807957e-05,\n",
    "        1.98842057e-05,\n",
    "    ]\n",
    ")\n",
    "\n",
    "mean_probs = np.array(list(get_mean_probs(df_fake, gru_model).values()))\n",
    "non_nan_idx = np.argwhere(~np.isnan(mean_probs)).flatten()\n",
    "non_nan_mean_prob = mean_probs[non_nan_idx]\n",
    "\n",
    "assert np.isclose(non_nan_mean_prob, NON_NAN_PROBS_MEAN, atol=1.0e-6).all()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# export\n",
    "def find_parens(toks: List[str], opening: str, closing: str) -> Dict[int, int]:\n",
    "    \"\"\"\n",
    "    Get the indices for the opening and closing tokens.\n",
    "    From https://stackoverflow.com/a/29992065/5768407\n",
    "    by user Baltasarq (https://stackoverflow.com/users/266978/baltasarq).\n",
    "\n",
    "    :param toks: the tokenized version of a method\n",
    "    :param opening: the opening token that will be matched against the closing token\n",
    "    :param closing: the closing token that will be matched against the opening token\n",
    "    :returns: returns a dictionary with the opening token indices as the keys and the closing token indices as the values\n",
    "    \"\"\"\n",
    "    toret = {}\n",
    "    pstack = []\n",
    "\n",
    "    for i, tok in enumerate(toks):\n",
    "        if tok == opening:\n",
    "            pstack.append(i)\n",
    "        elif tok == closing:\n",
    "            if len(pstack) == 0:\n",
    "                raise IndexError(\"No matching closing parens at: \" + str(i))\n",
    "            toret[pstack.pop()] = i\n",
    "\n",
    "    if len(pstack) > 0:\n",
    "        raise IndexError(\"No matching opening parens at: \" + str(pstack.pop()))\n",
    "\n",
    "    return toret\n",
    "\n",
    "\n",
    "def _get_dist_probs(\n",
    "    mthd: str, model: Model, opening: str, closing: str\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Get the distances and mean probabilities between opening and closing tokens in a given method.\n",
    "\n",
    "    :param mthd: the method to get the ranges of the opening and closing tokens and their probabilities\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param opening: the opening token used for calculating the distance between opening and closing tokens\n",
    "    :param closing: the closing token used for calculating the distance between opening and closing tokens as well as the token to get the mean probability of\n",
    "    :returns: returns a dictionary with the distance between the opening and closing tokens as keys and their mean probabilities as values\n",
    "    \"\"\"\n",
    "    # WARNING: Careful when using different tokenizers since HF tokenizers lib have diff API then HF transformers lib tokenizers... You will need to update this when using custom model and tokenizer...\n",
    "\n",
    "    # get the distances for the opening and closing tokens\n",
    "    toks = model.tokenizer.encode(mthd).tokens\n",
    "    idxs = find_parens(toks, opening, closing)\n",
    "\n",
    "    # get the model probabilities for the given method\n",
    "    inputs = model.tokenize(mthd)\n",
    "    probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    # sum up the probabilities of the different distances for the closing token\n",
    "    dist_probs = defaultdict(float)\n",
    "    for open_id, close_id in idxs.items():\n",
    "        dist_probs[close_id - open_id] += probs[close_id][\n",
    "            inputs[\"input_ids\"][0][close_id]\n",
    "        ]\n",
    "\n",
    "    # get the mean of the summed probabilities\n",
    "    dist_cnts = Counter([close_id - open_id for open_id, close_id in idxs.items()])\n",
    "    dist_probs = {dist: dist_probs[dist] / n for dist, n in dist_cnts.items()}\n",
    "    return dist_probs\n",
    "\n",
    "\n",
    "def mean_dist_probs(\n",
    "    df: pd.DataFrame,\n",
    "    model: Model,\n",
    "    opening: Optional[str] = \"<{>\",\n",
    "    closing: Optional[str] = \"<}>\",\n",
    "    n: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the distance between opening and closing tokens and the mean probability of each closing token that the model should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param opening: the opening token used for calculating the distance between opening and closing tokens\n",
    "    :param closing: the closing token used for calculating the distance between opening and closing tokens as well as the token to get the mean probability of\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a dataframe with the distances between opening and closing tokens and their mean probabilities\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # get the probabilities for the different distances for an entire dataframe\n",
    "    df = df.iloc[:n].copy()\n",
    "    dist_probs = df.code.apply(\n",
    "        lambda mthd: _get_dist_probs(mthd, model, opening, closing)\n",
    "    ).values\n",
    "\n",
    "    # flatten the keys of the different distances into a list\n",
    "    dist_keys = []\n",
    "    for probs in dist_probs:\n",
    "        dist_keys.extend(probs.keys())\n",
    "    # merge dictionaries across methods by taking the mean of probs with the same distance. Modified from https://stackoverflow.com/a/10461916/5768407,\n",
    "    # users georg https://stackoverflow.com/users/989121/georg and Rémy Hosseinkhan Boucher https://stackoverflow.com/users/12149730/r%c3%a9my-hosseinkhan-boucher\n",
    "    mean_dist_probs = {\n",
    "        k: np.nanmean(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    std_dist_probs = {\n",
    "        k: np.nanstd(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "\n",
    "    med_dist_probs = {\n",
    "        k: np.nanmedian(np.array([probs.get(k, np.nan) for probs in dist_probs]))\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    mad_dist_probs = {\n",
    "        k: stats.median_abs_deviation(\n",
    "            np.array([probs.get(k, np.nan) for probs in dist_probs]), nan_policy=\"omit\"\n",
    "        )\n",
    "        for k in set(dist_keys)\n",
    "    }\n",
    "    # TODO: convert to dictionary\n",
    "    df_dist = (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"dist\": list(mean_dist_probs.keys()),\n",
    "                \"mean_prob\": list(mean_dist_probs.values()),\n",
    "                \"std_prob\": list(std_dist_probs.values()),\n",
    "                \"med_prob\": list(med_dist_probs.values()),\n",
    "                \"mad_prob\": list(mad_dist_probs.values()),\n",
    "            }\n",
    "        )\n",
    "        .sort_values(\"dist\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df_dist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DIST_DF = pd.DataFrame(\n",
    "    {\n",
    "        \"dist\": [6, 10, 16],\n",
    "        \"mean_prob\": [\n",
    "            1.98822217e-05,\n",
    "            1.97613608e-05,\n",
    "            1.97816771e-05,\n",
    "        ],\n",
    "        \"std_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "        \"med_prob\": [\n",
    "            2.04683793e-05,\n",
    "            2.07205376e-05,\n",
    "            1.97817026e-05,\n",
    "        ],\n",
    "        \"mad_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "df_dist = mean_dist_probs(df_fake, gru_model, opening=\"(\", closing=\")\")\n",
    "\n",
    "assert (DIST_DF.dist.values == df_dist.dist.values).all()\n",
    "assert np.isclose(DIST_DF.mean_prob.values, df_dist.mean_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.std_prob.values, df_dist.std_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.med_prob.values, df_dist.med_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.mad_prob.values, df_dist.mad_prob.values, atol=1.0e-6).all()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DIST_DF = pd.DataFrame(\n",
    "    {\n",
    "        \"dist\": [6, 10, 16],\n",
    "        \"mean_prob\": [\n",
    "            1.98822217e-05,\n",
    "            1.97613608e-05,\n",
    "            1.97816771e-05,\n",
    "        ],\n",
    "        \"std_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "        \"med_prob\": [\n",
    "            2.04683793e-05,\n",
    "            2.07205376e-05,\n",
    "            1.97817026e-05,\n",
    "        ],\n",
    "        \"mad_prob\": [\n",
    "            4.93400876e-09,\n",
    "            0.00000000e00,\n",
    "            0.00000000e00,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "df_dist = mean_dist_probs(df_fake, trnsfr_model, opening=\"(\", closing=\")\")\n",
    "\n",
    "assert (DIST_DF.dist.values == df_dist.dist.values).all()\n",
    "assert np.isclose(DIST_DF.mean_prob.values, df_dist.mean_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.std_prob.values, df_dist.std_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.med_prob.values, df_dist.med_prob.values, atol=1.0e-6).all()\n",
    "assert np.isclose(DIST_DF.mad_prob.values, df_dist.mad_prob.values, atol=1.0e-6).all()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# export\n",
    "token_taxonomy = {\n",
    "  \"blocks\": {\n",
    "    \"<{>\": \"{\",\n",
    "    \"<}>\": \"}\",\n",
    "    \"<[>\": \"[\",\n",
    "    \"<]>\": \"]\",\n",
    "    \"<(>\": \"(\",\n",
    "    \"<)>\": \")\",\n",
    "    \"<;>\": \";\",\n",
    "    \"<return>\": \"return\"\n",
    "  },\n",
    "  \"exceptions\": {\n",
    "    \"<catch>\": \"catch\",\n",
    "    \"<try>\": \"try\",\n",
    "    \"<finally>\": \"finally\",\n",
    "    \"<throw>\": \"throw\",\n",
    "    \"<throws>\": \"throws\"\n",
    "  },\n",
    "  \"oop\": {\n",
    "    \"<class>\": \"class\",\n",
    "    \"<instanceof>\": \"instanceof\",\n",
    "    \"<interface>\": \"interface\",\n",
    "    \"<private>\": \"private\",\n",
    "    \"<protected>\": \"protected\",\n",
    "    \"<public>\": \"public\",\n",
    "    \"<abstract>\": \"abstract\",\n",
    "    \"<extends>\": \"extends\",\n",
    "    \"<package>\": \"package\",\n",
    "    \"<this>\": \"this\",\n",
    "    \"<implements>\": \"implements\",\n",
    "    \"<import>\": \"import\",\n",
    "    \"<new>\": \"new\",\n",
    "    \"<super>\": \"super\"\n",
    "  },\n",
    "  \"tests\": {\n",
    "    \"<assert>\": \"assert\"\n",
    "  },\n",
    "  \"declarations\": {\n",
    "    \"<native>\": \"native\",\n",
    "    \"<static>\": \"static\",\n",
    "    \"<synchronized>\": \"synchronized\",\n",
    "    \"<transient>\": \"transient\",\n",
    "    \"<volatile>\": \"volatile\",\n",
    "    \"<void>\": \"void\",\n",
    "    \"<final>\": \"final\",\n",
    "    \"<enum>\": \"enum\"\n",
    "  },\n",
    "  \"conditionals\": {\n",
    "    \"<else>\": \"else\",\n",
    "    \"<if>\": \"if\",\n",
    "    \"<switch>\": \"switch\",\n",
    "    \"<case>\": \"case\",\n",
    "    \"<default>\": \"default\"\n",
    "  },\n",
    "  \"loops\": {\n",
    "    \"<break>\": \"break\",\n",
    "    \"<do>\": \"do\",\n",
    "    \"<for>\": \"for\",\n",
    "    \"<while>\": \"while\",\n",
    "    \"<continue>\": \"continue\"\n",
    "  },\n",
    "  \"operators\": {\n",
    "    \"<=>\": \"=\",\n",
    "    \"<+>\": \"+\",\n",
    "    \"<->\": \"-\",\n",
    "    \"<*>\": \"*\",\n",
    "    \"</>\": \"/\",\n",
    "    \"<%>\": \"%\",\n",
    "    \"<++>\": \"++\",\n",
    "    \"<-->\": \"--\",\n",
    "    \"<!>\": \"!\",\n",
    "    \"<==>\": \"==\",\n",
    "    \"<!=>\": \"!=\",\n",
    "    \"<greater_equal>\": \">=\",\n",
    "    \"<lesser_equal>\": \"<=\",\n",
    "    \"<&&>\": \"&&\",\n",
    "    \"<||>\": \"||\",\n",
    "    \"<?>\": \"?\",\n",
    "    \"<:>\": \":\",\n",
    "    \"<~>\": \"~\",\n",
    "    \"<double_lesser>\": \"<<\",\n",
    "    \"<double_greater>\": \">>\",\n",
    "    \"<triple_greater>\": \">>>\",\n",
    "    \"<&>\": \"&\",\n",
    "    \"<^>\": \"^\",\n",
    "    \"<|>\": \"|\"\n",
    "  },\n",
    "  \"datatypes\": {\n",
    "    \"<byte>\": \"byte\",\n",
    "    \"<char>\": \"char\",\n",
    "    \"<float>\": \"float\",\n",
    "    \"<boolean>\": \"boolean\",\n",
    "    \"<double>\": \"double\",\n",
    "    \"<int>\": \"int\",\n",
    "    \"<long>\": \"long\",\n",
    "    \"<short>\": \"short\",\n",
    "    \"<strictfp>\": \"strictfp\"\n",
    "  },\n",
    "  \"extra_tokens\": {\n",
    "    \"<@>\": \"@\",\n",
    "    \"<...>\": \"...\",\n",
    "    \"<null>\": \"null\",\n",
    "    \"<true>\": \"true\",\n",
    "    \"<false>\": \"false\",\n",
    "    \"<n>\": \"\\n\"\n",
    "  }\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# export\n",
    "non_wordy = [\"<n>\", \"<...>\", \"<@>\", *token_taxonomy[\"operators\"], *token_taxonomy[\"blocks\"]]\n",
    "non_wordy.remove(\"<return>\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# export\n",
    "def get_error_rates(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    cnts = [0] * model.tokenizer.get_vocab_size()\n",
    "    err_cnts = [0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            cnts[idx] += 1\n",
    "            err_cnts[idx] += 1\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    cnts = np.array(cnts)\n",
    "    err_cnts = np.array(err_cnts)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(cnts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_errs = np.divide(err_cnts, cnts, out=nans, where=cnts != 0)\n",
    "    \n",
    "    error_taxonomy = token_taxonomy.copy()\n",
    "    \n",
    "    for cat, tokens in error_taxonomy.items():\n",
    "        errs = []\n",
    "        cnt_sum = 0\n",
    "        for token, keyword in tokens.items():\n",
    "            idx = model.tokenizer.token_to_id(token)\n",
    "            error_taxonomy[cat][token] = {\"error_rate\": mean_errs[idx], \"count\": cnts[idx]}\n",
    "            errs.append(mean_errs[idx])\n",
    "            cnt_sum += cnts[idx]\n",
    "\n",
    "        errs = np.array(errs)\n",
    "        error_taxonomy[cat][\"stats\"] = {\n",
    "            \"mean_error_rate\": np.nanmean(errs),\n",
    "            \"stdev_error_rate\": np.nanstd(errs),\n",
    "            \"median_error_rate\": np.nanmedian(errs),\n",
    "            \"mad_error_rate\": stats.median_abs_deviation(errs, nan_policy=\"omit\"),\n",
    "        }\n",
    "    \n",
    "    return error_taxonomy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :10\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/gru_layers1_vocab10000_embed256_units512\")\n",
    "err_tax = get_error_rates(df_buggy, model)\n",
    "err_tax"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# export\n",
    "def get_error_rates_df(df: pd.DataFrame, model: Model, bs: int = 16, n: Optional[int] = None):\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    rows = []\n",
    "    # loop through each method\n",
    "    for i in tqdm(range(0, n, bs), desc=\"Error Rates\", total = (n // bs) + 1):\n",
    "        batch = [\"<sos>\" + mthd for mthd in df.code.values[i:i + bs]]\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = np.stack([x.ids for x in model.tokenizer.encode_batch(batch)], axis = 0)\n",
    "        probs = model.get_probs(inputs)\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            row = {\n",
    "                \"y_\" + k: np.array([0.] * model.tokenizer.get_vocab_size())\n",
    "                for k in token_taxonomy.keys()\n",
    "            }\n",
    "            row_cnt = {\n",
    "                \"y_\" + k: np.array([0] * model.tokenizer.get_vocab_size())\n",
    "                for k in token_taxonomy.keys()\n",
    "            }\n",
    "            # loop through each token and its probability and update the container lists\n",
    "            for j, (idx, p) in enumerate(zip(inputs[i], probs[i])):\n",
    "                tok = model.tokenizer.id_to_token(idx)\n",
    "                for k in token_taxonomy:\n",
    "                    if tok in token_taxonomy[k]:\n",
    "                        # Check if token is wordy and could be part of variable or method\n",
    "                        if tok not in non_wordy:\n",
    "                            # Get the token version of the token behind the token under study\n",
    "                            # and check if the last character in the token contains a letter\n",
    "                            inp_tok_prev = model.tokenizer.id_to_token(inputs[i][j - 1])\n",
    "                            if re.search('[a-zA-Z]', inp_tok_prev[-1]):\n",
    "                                break\n",
    "                            # Check if there is a token infront of the token under study\n",
    "                            # if there is, get the token version of it\n",
    "                            # and check if the first character in the token contains a letter\n",
    "                            if j + 1 < len(inputs[i]):\n",
    "                                inp_tok_next = model.tokenizer.id_to_token(inputs[i][j + 1])\n",
    "                                if re.search('[a-zA-Z]', inp_tok_next[0]):\n",
    "                                    break\n",
    "                        row[\"y_\" + k][idx] += p[idx]\n",
    "                        row_cnt[\"y_\" + k][idx] += 1\n",
    "\n",
    "            for k in row:\n",
    "                # Check if there were no tokens found in this method for this particular taxonomy category\n",
    "                if not row_cnt[k].any():\n",
    "                    row[k] = np.nan\n",
    "                else:\n",
    "                    sum_cnt = np.sum(row_cnt[k])\n",
    "                    row[k] = np.sum(row[k]) / sum_cnt\n",
    "\n",
    "            rows.append(row)\n",
    "        \n",
    "    error_df = pd.DataFrame(rows)\n",
    "    error_df[\"original_code\"] = replace_spec_toks_to_original(df, java_special_tokens, n).code.values\n",
    "    error_df[\"transformed_code\"] = df.code.values[:n]\n",
    "    \n",
    "    return error_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :100\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "err_tax = get_error_rates_df(df_buggy, model)\n",
    "err_tax.describe()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=7.0, style=ProgressStyle(description_wi…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56437e62730148648c442ab759ce927a",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         y_blocks  y_exceptions         y_oop  y_tests  y_declarations  \\\n",
       "count  100.000000  2.800000e+01  9.800000e+01      0.0    6.400000e+01   \n",
       "mean     0.030134  6.314229e-06  1.747945e-03      NaN    1.479936e-06   \n",
       "std      0.019572  7.585470e-06  3.444690e-03      NaN    8.380155e-06   \n",
       "min      0.002529  1.614634e-08  1.236387e-08      NaN    3.552259e-08   \n",
       "25%      0.016589  1.111324e-06  8.302875e-08      NaN    1.712802e-07   \n",
       "50%      0.025835  2.241509e-06  1.344181e-05      NaN    3.463187e-07   \n",
       "75%      0.040499  1.035260e-05  2.141599e-03      NaN    3.845131e-07   \n",
       "max      0.087365  2.462625e-05  2.073873e-02      NaN    6.730417e-05   \n",
       "\n",
       "       y_conditionals       y_loops   y_operators   y_datatypes  \\\n",
       "count       57.000000  1.900000e+01  9.700000e+01  4.100000e+01   \n",
       "mean         0.000023  8.963920e-06  1.727996e-03  1.199283e-05   \n",
       "std          0.000025  6.504990e-06  5.010473e-03  1.776130e-05   \n",
       "min          0.000005  4.059797e-07  6.955367e-07  2.637347e-08   \n",
       "25%          0.000009  4.271065e-06  1.845865e-04  8.623591e-08   \n",
       "50%          0.000013  7.019739e-06  4.883553e-04  3.648138e-06   \n",
       "75%          0.000025  1.417009e-05  1.127912e-03  1.580466e-05   \n",
       "max          0.000137  1.905133e-05  4.579088e-02  7.690572e-05   \n",
       "\n",
       "       y_extra_tokens  \n",
       "count      100.000000  \n",
       "mean         0.130223  \n",
       "std          0.039746  \n",
       "min          0.010788  \n",
       "25%          0.103158  \n",
       "50%          0.130483  \n",
       "75%          0.158202  \n",
       "max          0.224735  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_blocks</th>\n",
       "      <th>y_exceptions</th>\n",
       "      <th>y_oop</th>\n",
       "      <th>y_tests</th>\n",
       "      <th>y_declarations</th>\n",
       "      <th>y_conditionals</th>\n",
       "      <th>y_loops</th>\n",
       "      <th>y_operators</th>\n",
       "      <th>y_datatypes</th>\n",
       "      <th>y_extra_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>9.800000e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>9.700000e+01</td>\n",
       "      <td>4.100000e+01</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.030134</td>\n",
       "      <td>6.314229e-06</td>\n",
       "      <td>1.747945e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.479936e-06</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>8.963920e-06</td>\n",
       "      <td>1.727996e-03</td>\n",
       "      <td>1.199283e-05</td>\n",
       "      <td>0.130223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.019572</td>\n",
       "      <td>7.585470e-06</td>\n",
       "      <td>3.444690e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.380155e-06</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>6.504990e-06</td>\n",
       "      <td>5.010473e-03</td>\n",
       "      <td>1.776130e-05</td>\n",
       "      <td>0.039746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.002529</td>\n",
       "      <td>1.614634e-08</td>\n",
       "      <td>1.236387e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.552259e-08</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4.059797e-07</td>\n",
       "      <td>6.955367e-07</td>\n",
       "      <td>2.637347e-08</td>\n",
       "      <td>0.010788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.016589</td>\n",
       "      <td>1.111324e-06</td>\n",
       "      <td>8.302875e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.712802e-07</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.271065e-06</td>\n",
       "      <td>1.845865e-04</td>\n",
       "      <td>8.623591e-08</td>\n",
       "      <td>0.103158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.025835</td>\n",
       "      <td>2.241509e-06</td>\n",
       "      <td>1.344181e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.463187e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>7.019739e-06</td>\n",
       "      <td>4.883553e-04</td>\n",
       "      <td>3.648138e-06</td>\n",
       "      <td>0.130483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.040499</td>\n",
       "      <td>1.035260e-05</td>\n",
       "      <td>2.141599e-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.845131e-07</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>1.417009e-05</td>\n",
       "      <td>1.127912e-03</td>\n",
       "      <td>1.580466e-05</td>\n",
       "      <td>0.158202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.087365</td>\n",
       "      <td>2.462625e-05</td>\n",
       "      <td>2.073873e-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.730417e-05</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>1.905133e-05</td>\n",
       "      <td>4.579088e-02</td>\n",
       "      <td>7.690572e-05</td>\n",
       "      <td>0.224735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": null
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "err_tax.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   y_blocks  y_exceptions         y_oop  y_tests  y_declarations  \\\n",
       "0  0.016703           NaN  8.925746e-06      NaN    3.519077e-07   \n",
       "1  0.019511           NaN  1.236387e-08      NaN    3.519077e-07   \n",
       "2  0.014283           NaN  1.757487e-04      NaN    3.093981e-07   \n",
       "3  0.025020           NaN  8.302875e-08      NaN             NaN   \n",
       "4  0.070037           NaN  2.955031e-04      NaN    1.820177e-07   \n",
       "\n",
       "   y_conditionals   y_loops  y_operators   y_datatypes  y_extra_tokens  \\\n",
       "0             NaN       NaN     0.000315           NaN        0.189852   \n",
       "1         0.00001       NaN     0.004352           NaN        0.131604   \n",
       "2             NaN       NaN     0.000488  2.802167e-05        0.162338   \n",
       "3         0.00001  0.000006     0.000996  2.637347e-08        0.097829   \n",
       "4             NaN       NaN     0.000131           NaN        0.124276   \n",
       "\n",
       "                                    transformed_code  \\\n",
       "0  <private> <void> success<(>io.netty.channel.Ch...   \n",
       "1  <private> <void> handleConnectRequest<(>com.as...   \n",
       "2  <@>java.lang.Override<n><protected> <void> onS...   \n",
       "3  <public> <boolean> <do>esEdgesHaveWeight<(><)>...   \n",
       "4  <@>org.junit.Test<n><public> <void> configures...   \n",
       "\n",
       "                                       original_code  \n",
       "0  private void success(io.netty.channel.Channel ...  \n",
       "1  private void handleConnectRequest(com.assistan...  \n",
       "2  @java.lang.Override\\nprotected void onSizeChan...  \n",
       "3  public boolean doesEdgesHaveWeight() {\\n    if...  \n",
       "4  @org.junit.Test\\npublic void configuresMultipl...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_blocks</th>\n",
       "      <th>y_exceptions</th>\n",
       "      <th>y_oop</th>\n",
       "      <th>y_tests</th>\n",
       "      <th>y_declarations</th>\n",
       "      <th>y_conditionals</th>\n",
       "      <th>y_loops</th>\n",
       "      <th>y_operators</th>\n",
       "      <th>y_datatypes</th>\n",
       "      <th>y_extra_tokens</th>\n",
       "      <th>transformed_code</th>\n",
       "      <th>original_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.925746e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.519077e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189852</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>private void success(io.netty.channel.Channel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.236387e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.519077e-07</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.131604</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>private void handleConnectRequest(com.assistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.757487e-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.093981e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>2.802167e-05</td>\n",
       "      <td>0.162338</td>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>@java.lang.Override\\nprotected void onSizeChan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.302875e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>2.637347e-08</td>\n",
       "      <td>0.097829</td>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>public boolean doesEdgesHaveWeight() {\\n    if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.955031e-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.820177e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.124276</td>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>@org.junit.Test\\npublic void configuresMultipl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": null
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# export\n",
    "def get_last_token_error_df(df: pd.DataFrame, model: Model, bs: int = 16, n: Optional[int] = None):\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for probabilities of the last token\n",
    "    ending = []\n",
    "    row = defaultdict(list)\n",
    "    # loop through each batch of methods\n",
    "    for i in tqdm(range(0, n, bs), desc=\"Error Rates\", total = (n // bs) + 1):\n",
    "        # get the batch of methods\n",
    "        batch = [mthd for mthd in df.code.values[i:i + bs]]\n",
    "        # tokenize the batch and get the probabilities for each token from the model\n",
    "        inputs = model.batch_tokenize(batch)\n",
    "        probs = model.get_probs(inputs)\n",
    "        \n",
    "        # turn off padding so that we can get the idx of the last token\n",
    "        model.tokenizer.backend_tokenizer.no_padding()\n",
    "        idx_last_ids = [len(x.ids) - 1 for x in model.tokenizer.backend_tokenizer.encode_batch(batch)]\n",
    "        # loop through the ids of the last token\n",
    "        for bs_idx, tok_idx in enumerate(idx_last_ids):\n",
    "            # append the probability of the last token to the list\n",
    "            prob = probs[bs_idx][tok_idx][inputs[\"input_ids\"][bs_idx][tok_idx]].item()\n",
    "            ending.append(prob)\n",
    "    \n",
    "    # turn the list into a dataframe in the correct format\n",
    "    last_error_df = pd.DataFrame(ending, columns=[\"y_ending_error\"])\n",
    "    last_error_df[\"original_code\"] = replace_spec_toks_to_original(df, java_special_tokens, n).code.values\n",
    "    last_error_df[\"transformed_code\"] = df.code.values[:n]\n",
    "    \n",
    "    return last_error_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/testbed/ts-bug-fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :100\n",
    "]\n",
    "model = WildModel(\"EleutherAI/gpt-neo-125M\")\n",
    "# model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "last_tok_err_df = get_last_token_error_df(df_buggy, model)\n",
    "last_tok_err_df.describe()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Error Rates', max=7.0, style=ProgressStyle(description_wi…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7024628fc8904a5e8b3cc8fe3a47b2c8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       y_ending_error\n",
       "count       96.000000\n",
       "mean         0.045696\n",
       "std          0.078886\n",
       "min          0.000003\n",
       "25%          0.002425\n",
       "50%          0.016016\n",
       "75%          0.046613\n",
       "max          0.395484"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_ending_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.045696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.078886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.016016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.046613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.395484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# export\n",
    "def get_mean_cross_entropy(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean cross entropy for a model on an entire pandas dataframe\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns the mean cross entropy of the models predictions compared to true labels\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    cross_entropy_losses = []\n",
    "    # Need to change to sparse_categorical_crossentropy\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # calculate the cross entropy between the labels and probabilities\n",
    "        losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            inputs[\"input_ids\"], probs\n",
    "        ).numpy()\n",
    "        cross_entropy_losses.append(losses)\n",
    "\n",
    "    # flatten list of cross entropies and calculate the mean, median, std, and mad\n",
    "    cross_entropy_losses = np.concatenate(cross_entropy_losses)\n",
    "    return {\n",
    "        \"mean\": np.mean(cross_entropy_losses),\n",
    "        \"median\": np.median(cross_entropy_losses),\n",
    "        \"std\": np.std(cross_entropy_losses),\n",
    "        \"mad\": stats.median_abs_deviation(cross_entropy_losses),\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cross_entropy_losses = []\n",
    "for mthd in df_fake.code.values:\n",
    "    inputs = gru_model.tokenize(mthd)\n",
    "    probs = gru_model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        inputs[\"input_ids\"], probs\n",
    "    ).numpy()\n",
    "    cross_entropy_losses.append(losses)\n",
    "\n",
    "CROSS_ENTROPY_MEAN = np.mean(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MEDIAN = np.median(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_STD = np.std(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MAD = stats.median_abs_deviation(np.concatenate(cross_entropy_losses))\n",
    "cross_entropy = get_mean_cross_entropy(df_fake, gru_model)\n",
    "\n",
    "assert np.isclose(CROSS_ENTROPY_MEAN, cross_entropy[\"mean\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MEDIAN, cross_entropy[\"median\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_STD, cross_entropy[\"std\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MAD, cross_entropy[\"mad\"], atol=1.0e-6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cross_entropy_losses = []\n",
    "for mthd in df_fake.code.values:\n",
    "    inputs = trnsfr_model.tokenize(mthd)\n",
    "    probs = trnsfr_model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "    losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        inputs[\"input_ids\"], probs\n",
    "    ).numpy()\n",
    "    cross_entropy_losses.append(losses)\n",
    "\n",
    "CROSS_ENTROPY_MEAN = np.mean(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MEDIAN = np.median(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_STD = np.std(np.concatenate(cross_entropy_losses))\n",
    "CROSS_ENTROPY_MAD = stats.median_abs_deviation(np.concatenate(cross_entropy_losses))\n",
    "cross_entropy = get_mean_cross_entropy(df_fake, trnsfr_model)\n",
    "\n",
    "assert np.isclose(CROSS_ENTROPY_MEAN, cross_entropy[\"mean\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MEDIAN, cross_entropy[\"median\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_STD, cross_entropy[\"std\"], atol=1.0e-6)\n",
    "assert np.isclose(CROSS_ENTROPY_MAD, cross_entropy[\"mad\"], atol=1.0e-6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# export\n",
    "def get_mean_probs(df: pd.DataFrame, model: Model, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean probability of each token that the model\n",
    "    should predict for an entire pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a numpy array of the mean probability for each token in the model's vocab\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # setup container lists for the number of occurrences and sum of probabilities for each token\n",
    "    counts = [0] * model.tokenizer.get_vocab_size()\n",
    "    sum_probs = [0.0] * model.tokenizer.get_vocab_size()\n",
    "    # loop through each method\n",
    "    for mthd in df.code.values[:n]:\n",
    "        # token the method and generate the probabilities for the model's predictions\n",
    "        inputs = model.tokenize(mthd)\n",
    "        probs = model.get_probs(inputs)[0].numpy()\n",
    "\n",
    "        # loop through each token and its probability and update the container lists\n",
    "        for idx, p in zip(inputs[\"input_ids\"][0], probs):\n",
    "            counts[idx] += 1\n",
    "            sum_probs[idx] += p[idx]\n",
    "\n",
    "    # convert the lists to numpy lists and perform element wise division to get the mean probabilities for each token\n",
    "    counts = np.array(counts)\n",
    "    sum_probs = np.array(sum_probs)\n",
    "\n",
    "    # perform division, but not when denominator is zero. In those cases, just leave value as NAN.\n",
    "    nans = np.empty(counts.shape)\n",
    "    nans.fill(np.nan)\n",
    "    mean_probs = np.divide(sum_probs, counts, out=nans, where=counts != 0)\n",
    "    # TODO: convert to dictionary with keys as tokens\n",
    "    mean_probs = {\n",
    "        model.tokenizer.id_to_token(i): mean_probs[i] for i in range(len(mean_probs))\n",
    "    }\n",
    "    return mean_probs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# export\n",
    "def get_mean_cross_entropy_df(df: pd.DataFrame, model: Model, bs = 16, n: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Get the mean cross entropy for a model on an entire pandas dataframe\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the model predict on\n",
    "    :param model: the model used to generate the predictions\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns the mean cross entropy of the models predictions compared to true labels\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    cross_entropy_losses = []\n",
    "    for i in tqdm(range(0, n, bs), desc=\"Cross Entropies\", total = (n // bs) + 1):\n",
    "        batch = [\"<sos>\" + mthd for mthd in df.code.values[i:i + bs]]\n",
    "        # token the method and get the probabilities for each token from the model\n",
    "        inputs = np.stack([x.ids for x in model.tokenizer.encode_batch(batch)], axis = 0)\n",
    "        probs = model.get_probs(inputs)\n",
    "\n",
    "        # calculate the cross entropy between the labels and probabilities\n",
    "        losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            inputs, probs\n",
    "        ).numpy()\n",
    "        cross_entropy_losses.extend(np.mean(losses, axis = 1))\n",
    "    \n",
    "    new_df = pd.DataFrame(\n",
    "        zip(replace_spec_toks_to_original(df, java_special_tokens, n).code.values, df.code.values[:n], cross_entropy_losses),\n",
    "        columns=[\"original_code\", \"transformed_code\", \"y_cross_entropy\"]\n",
    "    )\n",
    "\n",
    "    return new_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from glob import glob\n",
    "\n",
    "path = Path(\"/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses\")\n",
    "for d in glob(str(path) + \"/*/\"):\n",
    "    if \"gru_layers3_vocab10000_embed256_units1024\" in d: continue\n",
    "    print(d)\n",
    "    bug_fix_cross_entropy = pd.read_json(\n",
    "        d + \"/bug_fix_cross_entropy.jsonl\", orient=\"records\", lines=True\n",
    "    )\n",
    "    bug_fix_cross_entropy[\"original_code\"] = replace_spec_toks_to_original(\n",
    "        bug_fix_cross_entropy, java_special_tokens\n",
    "    ).code.values\n",
    "    bug_fix_cross_entropy.rename(columns={\"code\": \"transformed_code\"}, inplace=True)\n",
    "    bug_fix_cross_entropy.to_json(d + \"/bug_fix_cross_entropy.jsonl\", orient=\"records\", lines=True)\n",
    "    \n",
    "#     bug_fix_error_taxonomy = pd.read_json(\n",
    "#         d + \"/bug_fix_error_taxonomy.jsonl\", orient=\"records\", lines=True\n",
    "#     )\n",
    "#     bug_fix_error_taxonomy[\"original_code\"] = replace_spec_toks_to_original(\n",
    "#         bug_fix_error_taxonomy, java_special_tokens\n",
    "#     ).code.values\n",
    "#     bug_fix_error_taxonomy.rename(columns={\"code\": \"transformed_code\"}, inplace=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/gru_layers1_vocab10000_embed256_units2048/\n",
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/gru_layers2_vocab10000_embed256_units1024/\n",
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/rnn_layers1_vocab10000_embed256_units1024/\n",
      "/home/jovyan/work/dvc-icodegen/nbs/nbs_experiments/results/analyses/gru_layers1_vocab10000_embed256_units512/\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/datasets/controlled/testbeds/_ts_bug_fix\")\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[\n",
    "    :100\n",
    "]\n",
    "model = RNNModel.from_path(\"/home/jovyan/work/dvc-icodegen/models/controlled/rnns/rnn_layers1_vocab10000_embed256_units1024\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cross_entropy = get_mean_cross_entropy_df(df_buggy, model)\n",
    "cross_entropy.describe()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cross Entropies', max=7.0, style=ProgressStyle(descriptio…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d846b7d302c4170b89deb9b7005415b",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       y_cross_entropy\n",
       "count       100.000000\n",
       "mean          6.036340\n",
       "std           1.529498\n",
       "min           3.113172\n",
       "25%           4.934342\n",
       "50%           5.905706\n",
       "75%           6.866514\n",
       "max           9.877866"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_cross_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.036340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.529498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.113172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.934342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.905706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.866514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.877866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": null
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cross_entropy.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                       original_code  \\\n",
       "0  private void success(io.netty.channel.Channel ...   \n",
       "1  private void handleConnectRequest(com.assistan...   \n",
       "2  @java.lang.Override\\nprotected void onSizeChan...   \n",
       "3  public boolean doesEdgesHaveWeight() {\\n    if...   \n",
       "4  @org.junit.Test\\npublic void configuresMultipl...   \n",
       "\n",
       "                                    transformed_code  y_cross_entropy  \n",
       "0  <private> <void> success<(>io.netty.channel.Ch...         6.006714  \n",
       "1  <private> <void> handleConnectRequest<(>com.as...         6.368460  \n",
       "2  <@>java.lang.Override<n><protected> <void> onS...         3.927362  \n",
       "3  <public> <boolean> <do>esEdgesHaveWeight<(><)>...         3.711157  \n",
       "4  <@>org.junit.Test<n><public> <void> configures...         6.497345  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_code</th>\n",
       "      <th>transformed_code</th>\n",
       "      <th>y_cross_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private void success(io.netty.channel.Channel ...</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; success&lt;(&gt;io.netty.channel.Ch...</td>\n",
       "      <td>6.006714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private void handleConnectRequest(com.assistan...</td>\n",
       "      <td>&lt;private&gt; &lt;void&gt; handleConnectRequest&lt;(&gt;com.as...</td>\n",
       "      <td>6.368460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@java.lang.Override\\nprotected void onSizeChan...</td>\n",
       "      <td>&lt;@&gt;java.lang.Override&lt;n&gt;&lt;protected&gt; &lt;void&gt; onS...</td>\n",
       "      <td>3.927362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public boolean doesEdgesHaveWeight() {\\n    if...</td>\n",
       "      <td>&lt;public&gt; &lt;boolean&gt; &lt;do&gt;esEdgesHaveWeight&lt;(&gt;&lt;)&gt;...</td>\n",
       "      <td>3.711157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@org.junit.Test\\npublic void configuresMultipl...</td>\n",
       "      <td>&lt;@&gt;org.junit.Test&lt;n&gt;&lt;public&gt; &lt;void&gt; configures...</td>\n",
       "      <td>6.497345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": null
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# export\n",
    "TYPES = 3\n",
    "\n",
    "def _get_metrics(df, model):\n",
    "    error_taxonomy_df = get_error_rates_df(df, model, bs = 192)\n",
    "    # mean_cross_entropy_df = get_mean_cross_entropy_df(df, model, bs = 192)\n",
    "\n",
    "    return {\n",
    "        \"error_taxonomy\": error_taxonomy_df,\n",
    "        # \"mean_cross_entropy\": mean_cross_entropy_df,\n",
    "    }\n",
    "\n",
    "\n",
    "def save_results(control_df, treatment_df, model, err_path, cross_path):\n",
    "    control_metrics = _get_metrics(control_df, model)\n",
    "    treatment_metrics = _get_metrics(treatment_df, model)\n",
    "    \n",
    "    err_df = pd.concat(\n",
    "        [control_metrics[\"error_taxonomy\"], treatment_metrics[\"error_taxonomy\"]]\n",
    "    ).sort_index().reset_index(drop=True)\n",
    "    err_df[\"x_treatment\"] = [False, True] * len(control_metrics[\"error_taxonomy\"])\n",
    "    err_df.to_json(err_path, orient=\"records\", lines=True)\n",
    "\n",
    "    # cross_df = pd.concat(\n",
    "    #     [control_metrics[\"mean_cross_entropy\"], treatment_metrics[\"mean_cross_entropy\"]]\n",
    "    # ).sort_index().reset_index(drop=True)\n",
    "    # cross_df[\"x_treatment\"] = [False, True] * len(control_metrics[\"mean_cross_entropy\"])\n",
    "    # cross_df.to_json(cross_path, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "def _long_range(bigclone_path, bugfix_path, cmt_path, model, out_path, n=None):\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i in range(1, TYPES + 1):\n",
    "        df = pd.read_json(bigclone_path / f\"bigclone-type-{i}.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "        control_df = df[\"function_1\"].to_frame().rename(columns={\"function_1\": \"code\"})\n",
    "        treatment_df = df[\"function_2\"].to_frame().rename(columns={\"function_2\": \"code\"})\n",
    "        \n",
    "        err_path = out_path / f\"bigclone_type_{i}_error_taxonomy.jsonl\"\n",
    "        cross_path = out_path / f\"bigclone_type_{i}_cross_entropy.jsonl\"\n",
    "        save_results(control_df, treatment_df, model, err_path, cross_path)\n",
    "    \n",
    "    control_df = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "    treatment_df = pd.read_json(bugfix_path / \"fixed.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "\n",
    "    err_path = out_path / f\"bug_fix_error_taxonomy.jsonl\"\n",
    "    cross_path = out_path / f\"bug_fix_cross_entropy.jsonl\"\n",
    "    save_results(control_df, treatment_df, model, err_path, cross_path)\n",
    "    \n",
    "    control_df = pd.read_json(cmt_path / \"uncommented_code.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "    treatment_df = pd.read_json(cmt_path / \"commented_code.jsonl\", orient=\"records\", lines=True)[:n]\n",
    "    \n",
    "    err_path = out_path / f\"commenting_error_taxonomy.jsonl\"\n",
    "    cross_path = out_path / f\"commenting_cross_entropy.jsonl\"\n",
    "    save_results(control_df, treatment_df, model, err_path, cross_path)\n",
    "\n",
    "\n",
    "def evaluate(data_path, model_path, experiment_path):\n",
    "    \"\"\"Function for evaluating models related to the library.\"\"\"\n",
    "    results = defaultdict(dict)\n",
    "    testbed_path = data_path / \"testbed\"\n",
    "    \n",
    "    # These model folders will need to contain the config of the model as well\n",
    "    # to differentiate them\n",
    "    for m_path in model_path.glob(\"*/\"):\n",
    "        model = None\n",
    "        if \"dvc\" in m_path.name:\n",
    "            continue\n",
    "        elif \".gitignore\" in m_path.name:\n",
    "            continue\n",
    "        elif \"deprecated\" in m_path.name:\n",
    "            continue\n",
    "        \n",
    "        print(m_path)\n",
    "        if \"tfr\" in m_path.name:\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "            model = TransformerHFModel.from_path(m_path, TFGPT2LMHeadModel, optimizer, _loss)\n",
    "        else:\n",
    "            model = RNNModel.from_path(m_path)\n",
    "\n",
    "        bigclone_path = testbed_path / \"ts-bigclone-types\"\n",
    "        bugfix_path = testbed_path / \"ts-bug-fix\"\n",
    "        cmt_path = testbed_path / \"ts-comments\"\n",
    "\n",
    "        # Long-Range Interactions\n",
    "        _long_range(\n",
    "            bigclone_path, bugfix_path, cmt_path,\n",
    "            model, experiment_path / m_path.name\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = Path(\"/home/jovyan/work\")\n",
    "data_path = path / \"dvc-icodegen\"\n",
    "model_path = path / \"dvc-icodegen/models/controlled/rnns/\"\n",
    "experiment_path = path / \"dvc-icodegen/nbs/nbs_experiments/results/analyses/rnns\"\n",
    "model = evaluate(data_path, model_path, experiment_path)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 00_data.core.ipynb.\n",
      "Converted 01_data.transforms.ipynb.\n",
      "Converted 02_model.core.ipynb.\n",
      "Converted 04_evaluation.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}