{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# default_exp model.core"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "> API details. @nathan"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from icodegen.data.core import convert_df_to_tfds, java_special_tokens, train_tokenizer\n",
    "from pathlib import Path, PurePath\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import datetime\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, TFGPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from icodegen.data.core import convert_df_to_tfds, java_special_tokens, train_tokenizer, replace_spec_toks_to_original\n",
    "from typing import Optional, Dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setup\n",
    "\n",
    "df_fake = pd.DataFrame(\n",
    "    [\"aaaa(bb(aaaa(bb()()ccc)dd)()ccc)dd\", \"aaaa(bb()ccccc)dd\"], columns=[\"code\"]\n",
    ")\n",
    "\n",
    "# Tokenize the data\n",
    "max_length = 16\n",
    "batch_size = 1\n",
    "vocab_sz = 100\n",
    "tokenizer = train_tokenizer(df_fake, java_special_tokens, max_length, vocab_sz=vocab_sz)\n",
    "dataset = convert_df_to_tfds(df_fake, tokenizer, max_length, batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Codesearchnet Java"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "java_df = pd.read_csv('/tf/main/dvc-icodegen/data/clean_java.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "java_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "java_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "java_df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "java_train = java_df[java_df['partition'] == 'train']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "java_train.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "java_trn_samples = java_train.sample(100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Tokenize the data\n",
    "\n",
    "max_length = 112\n",
    "batch_size = 16\n",
    "vocab_sz = 10000\n",
    "tokenizer = train_tokenizer(java_trn_samples, java_special_tokens, max_length, vocab_sz=vocab_sz)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "java_trn_dataset = convert_df_to_tfds(java_trn_samples, tokenizer, max_length, batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "java_trn_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer.get_vocab_size()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def _loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "class Model(ABC):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    @abstractmethod\n",
    "    def from_path(path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_probs(self, inputs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def tokenize(self, method):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, ds, epochs):\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "HuggingFace-based implementation, originally [this](!https://huggingface.co/transformers/model_doc/gpt2.html)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "class TransformerHFModel(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_path,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        optimizer,\n",
    "        loss,\n",
    "        name_sufix=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructs a custom transformer model using a provided HF model\n",
    "        :param out_path: \n",
    "        :param model: HF transformer model\n",
    "        :tokenizer: HF tokenizer model\n",
    "        :optimizer: Keras optimizer\n",
    "        :loss: Custom loss function for model training\n",
    "        \"\"\"\n",
    "        \n",
    "        # HuggingFace model implementation is left outside this class in order to\n",
    "        # decouple from any specific implementation, besides aiming experimentation with multiple models\n",
    "        \n",
    "        model_config = model.config\n",
    "        self.config_name = f\"tfr_layers{model_config.n_layer}_vocab{model_config.vocab_size}_embd{model_config.n_embd}_heads{model_config.n_head}\"\n",
    "        \n",
    "        if name_sufix is not None:\n",
    "            self.config_name += f\"_{name_sufix}\"\n",
    "        \n",
    "        pure_path = PurePath(str(out_path))\n",
    "        \n",
    "        if pure_path.name != self.config_name:\n",
    "            self.out_path = Path(out_path) / self.config_name\n",
    "        else:\n",
    "            self.out_path = Path(out_path)\n",
    "        \n",
    "        self.out_path.mkdir(exist_ok=True)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        \n",
    "        tensorboard_path = self.out_path / \"tensorboard_logs\"\n",
    "        tensorboard_path.mkdir(exist_ok=True)\n",
    "        tensorboard_path = tensorboard_path  / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        \n",
    "        self.callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=self.out_path / \"ckpt_{epoch}\", save_weights_only=True\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=str(tensorboard_path),\n",
    "                histogram_freq=0,  # How often to log histogram visualizations\n",
    "                embeddings_freq=0,  # How often to log embedding visualizations\n",
    "                update_freq=\"epoch\",\n",
    "            ),  # How often to write logs (default: once per epoch)\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                # Stop training when `val_loss` is no longer improving\n",
    "                monitor=\"val_loss\",\n",
    "                # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "                min_delta=1e-2,\n",
    "                # \"no longer improving\" being further defined as \"for at least 5 epochs\"\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "            ),\n",
    "        ]\n",
    "        \n",
    "        super().__init__(tokenizer, model)\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_path(model_path, hf_model, optimizer, loss):\n",
    "        path = Path(model_path)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = Tokenizer.from_file(str(path / \"tokenizer.json\"))\n",
    "        \n",
    "        # Load model\n",
    "        with open(path / \"model_config.json\", \"r\") as f:\n",
    "            model_config = json.load(f)\n",
    "        full_path = path / \"model\"\n",
    "        loaded_model = hf_model.from_pretrained(full_path)\n",
    "        \n",
    "        name_sufix = None\n",
    "        if \"_pretrained\" in str(full_path):\n",
    "            name_sufix = \"pretrained\"\n",
    "        \n",
    "        model = TransformerHFModel(model_path, loaded_model, tokenizer, optimizer, loss, name_sufix=name_sufix)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def save(self):\n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save(str(self.out_path / \"tokenizer.json\"), pretty=True)\n",
    "        # Save HF model\n",
    "        self.model.save_pretrained(self.out_path / \"model\")\n",
    "        \n",
    "        # Save model config.\n",
    "        model_config = self.model.config\n",
    "        # TODO: extract attributes from configuration\n",
    "        config_dict = {\n",
    "          \"activation_function\": model_config.activation_function,\n",
    "          \"attn_pdrop\": model_config.attn_pdrop,\n",
    "          \"bos_token_id\": model_config.bos_token_id,\n",
    "          \"embd_pdrop\": model_config.embd_pdrop,\n",
    "          \"eos_token_id\": model_config.eos_token_id,\n",
    "          \"gradient_checkpointing\": model_config.gradient_checkpointing,\n",
    "          \"initializer_range\": model_config.initializer_range,\n",
    "          \"layer_norm_epsilon\": model_config.layer_norm_epsilon,\n",
    "          \"model_type\": model_config.model_type,\n",
    "          \"n_ctx\": model_config.n_ctx,\n",
    "          \"n_embd\": model_config.n_embd,\n",
    "          \"n_head\": model_config.n_head,\n",
    "          \"n_inner\": model_config.n_inner,\n",
    "          \"n_layer\": model_config.n_layer,\n",
    "          \"n_positions\": model_config.n_positions,\n",
    "          \"pad_token_id\": model_config.pad_token_id,\n",
    "          \"resid_pdrop\": model_config.resid_pdrop,\n",
    "          \"summary_activation\": model_config.summary_activation,\n",
    "          \"summary_first_dropout\": model_config.summary_first_dropout,\n",
    "          \"summary_proj_to_labels\": model_config.summary_proj_to_labels,\n",
    "          \"summary_type\": model_config.summary_type,\n",
    "          \"summary_use_proj\": model_config.summary_use_proj,\n",
    "          \"vocab_size\": model_config.vocab_size\n",
    "        }\n",
    "        \n",
    "        with open(self.out_path / \"model_config.json\", \"w\") as file:\n",
    "            json.dump(config_dict, file)\n",
    "        \n",
    "    \n",
    "    def generate(self, max_length, n, top_k=10, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Performs generation process through sampling strategy (top-k) by default\n",
    "        :param max_length: Max. allowed length for generated sequences\n",
    "        :param n: Number of samples to generate\n",
    "        :param top_k: top-k most likely next words to be filtered for the sampling\n",
    "        :returns Generated sequences:\n",
    "        \"\"\"\n",
    "        \n",
    "        input_seed = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_ids = tf.expand_dims(input_seed, 0)\n",
    "        \n",
    "        gen_txt = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_k=top_k,\n",
    "            num_return_sequences=n\n",
    "        )\n",
    "        \n",
    "        generated = []\n",
    "        for i, sample in enumerate(gen_txt):\n",
    "            generated.append(self.tokenizer.decode(sample, skip_special_tokens=False))\n",
    "            \n",
    "        return generated\n",
    "    \n",
    "    def get_probs(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        logits = outputs[0]\n",
    "        probs = tf.nn.softmax(logits)\n",
    "\n",
    "        return probs    \n",
    "    \n",
    "    def sample_top_k(self, max_length, top_k=50):\n",
    "        \"\"\"\n",
    "        Performs sampling (generation) using top-k mechanism\n",
    "        :param max_length: Max. length allowed in the generation process\n",
    "        :param top_k: top-k most likely next words to be filtered for the sampling\n",
    "        :returns: Generated text\n",
    "        \"\"\"\n",
    "        input_seed = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_ids = tf.expand_dims(input_seed, 0)\n",
    "        \n",
    "        gen_txt = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(gen_txt[0], skip_special_tokens=False)\n",
    "    \n",
    "    def sample_nucleus(self, max_length, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Perform sampling using top-p (nucleus) mechanism\n",
    "        :param  max_length: Max. allowed length of generated sequences \n",
    "        :param top_p: Probability value to find possible set of words whose cumulative probability exceeds top-p value\n",
    "        :returns: Generated text\n",
    "        \"\"\"\n",
    "        input_seed = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_ids = tf.expand_dims(input_seed, 0)\n",
    "\n",
    "        gen_txt = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_p=top_p,\n",
    "            top_k=0\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.decode(gen_txt[0], skip_special_tokens=False)\n",
    "    \n",
    "    def sample_temperature(self, max_length, temperature=0.7):\n",
    "        \"\"\"\n",
    "        Performs sampling using temperature strategy\n",
    "        :param max_length: Max. allowed length of generated sequences \n",
    "        :param temperature: Temperature parameter of the softmax\n",
    "        :returns: Generated text\n",
    "        \"\"\"\n",
    "        input_seed = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_ids = tf.expand_dims(input_seed, 0)\n",
    "\n",
    "        gen_txt = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            top_k=0,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(gen_txt[0], skip_special_tokens=False)\n",
    "        \n",
    "\n",
    "    def tokenize(self, method):\n",
    "        \"\"\"\n",
    "        :param method: Code snippet (plain text)\n",
    "        :returns: Encoded result using the provided tokenizer\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        # encode method and then convert to format that hf models expect\n",
    "        encoding = self.tokenizer.encode(\"<sos>\" + method)\n",
    "        output[\"input_ids\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0\n",
    "        )\n",
    "        output[\"attention_mask\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "        # return self.tokenizer(method, return_tensors=\"tf\")\n",
    "\n",
    "    def train(self, ds_train, ds_val, epochs):\n",
    "        \"\"\"\n",
    "        Performs training process leveraging keras methods.\n",
    "        :param ds_train: Tensorflow dataset for training\n",
    "        :param ds_val: Tensorflow dataset for validation\n",
    "        :param epochs: Number of epochs to execute training\n",
    "        \"\"\"\n",
    "        # Be careful with the definition of the loss function when compiling\n",
    "        # Gotta consider the number of layers (n_layer param.) --> actually number of decoder blocks\n",
    "        self.model.compile(optimizer=self.optimizer, loss=[self.loss, *[None] * self.model.config.n_layer])\n",
    "        history = self.model.fit(ds_train, epochs=epochs, callbacks=self.callbacks, validation_data=ds_val)\n",
    "        \n",
    "        return history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test GPT2 - HuggingFace-based"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Establish configuration for HF model, in this case by means of GPT2Config class\n",
    "\n",
    "custom_config_gpt2 = GPT2Config(\n",
    "    vocab_size = tokenizer.get_vocab_size(),\n",
    "    bos_token_id = tokenizer.token_to_id(\"<sos>\"),\n",
    "    eos_token_id = tokenizer.token_to_id(\"<eos>\"),\n",
    "    pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Configuration\n",
    "\n",
    "gpt2_hf_model_path = \"models/GPT2-HF\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Implement the corresponding HuggingFace model using the custom configuration\n",
    "\n",
    "hf_gpt_model = TFGPT2LMHeadModel(custom_config_gpt2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gpt2_model = TransformerHFModel(gpt2_hf_model_path, hf_gpt_model, tokenizer, optimizer, _loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TRAIN_EPOCHS = 20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_trn_time = time.time()\n",
    "gpt2_model.train(dataset, dataset, epochs=TRAIN_EPOCHS)\n",
    "end_trn_time = time.time()\n",
    "print(f'Time elapsed training: {end_trn_time - start_trn_time} seconds.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NUMBER_GEN_SEQS = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_gen_time = time.time()\n",
    "generated_seqs = gpt2_model.generate(n=NUMBER_GEN_SEQS, max_length=max_length)\n",
    "end_gen_time = time.time()\n",
    "print(f'Time elapsed generating: {end_gen_time - start_gen_time} seconds.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for seq in generated_seqs:\n",
    "    print(f'sample: {seq}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gpt2_model.save()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loaded_gpt2_model = TransformerHFModel.from_path(\"models/GPT2-HF/tfgp_t2lm_head_model_4-20210330-171318\", TFGPT2LMHeadModel, optimizer, _loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type(loaded_gpt2_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generated_seqs = gpt2_model.generate(n=10, max_length=max_length)\n",
    "print(generated_seqs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = \"test\"\n",
    "encoded_text = tokenizer.encode(\"<sos>\" + text).ids\n",
    "input_eval = tf.expand_dims(encoded_text, 0)\n",
    "logits = gpt2_model.model(input_eval)[0].numpy()\n",
    "inputs = gpt2_model.tokenize(text)\n",
    "probs = gpt2_model.get_probs(inputs)[0].numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "probs.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for i in range(len(probs)):\n",
    "#     assert np.isclose(1.0, probs[i].sum())\n",
    "#     assert np.argmax(logits[i]) == np.argmax(probs[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gpt2_model.sample_nucleus(max_length=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gpt2_model.sample_top_k(max_length=20, top_k=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keras-based implementation (based on [this example](!https://keras.io/examples/generative/text_generation_with_miniature_gpt/#prepare-the-data-for-wordlevel-language-modelling))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Auxiliar classes / methods for building the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "    \n",
    "# export\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"maxlen\": self.maxlen,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "class MiniatureGPTModel(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_path, \n",
    "        max_length,\n",
    "        batch_size,\n",
    "        ff_dim,\n",
    "        embedding_dim,\n",
    "        n_heads,\n",
    "        n_transformer_blocks,\n",
    "        vocab_size,\n",
    "        tokenizer,\n",
    "        optimizer,\n",
    "        loss\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Builds a transformer model using tf-keras API\n",
    "        :param out_path: Path for saving model data\n",
    "        :param max_length: Max. length allowed for the sequences\n",
    "        :param batch_size: \n",
    "        :param ff_dim:\n",
    "        :param embedding_dim:\n",
    "        :param n_heads:\n",
    "        :param n_transformer_blocks:\n",
    "        :param vocab_size:\n",
    "        :param tokenizer:\n",
    "        :param optimizer:\n",
    "        :param loss:\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.ff_dim = ff_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_transformer_blocks = n_transformer_blocks\n",
    "        self.vocab_size = vocab_size\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.config_name = f\"min_tfr_layers{n_transformer_blocks}_vocab{tokenizer.get_vocab_size()}_embd{embedding_dim}_heads{n_heads}\"\n",
    "        \n",
    "        model = self.__create_model()\n",
    "        \n",
    "        model._name = \"Miniature-GPT\"\n",
    "        \n",
    "        pure_path = PurePath(str(out_path))\n",
    "        \n",
    "        if pure_path.name != self.config_name:\n",
    "            self.out_path = Path(out_path) / self.config_name\n",
    "        else:\n",
    "            self.out_path = Path(out_path)\n",
    "        \n",
    "        self.out_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        tensorboard_path = self.out_path / \"tensorboard_logs\"\n",
    "        tensorboard_path.mkdir(exist_ok=True)\n",
    "        tensorboard_path = tensorboard_path / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=self.out_path / \"ckpt_{epoch}\", save_weights_only=True\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=str(tensorboard_path),\n",
    "                histogram_freq=0,  # How often to log histogram visualizations\n",
    "                embeddings_freq=0,  # How often to log embedding visualizations\n",
    "                update_freq=\"epoch\",\n",
    "            ),  # How often to write logs (default: once per epoch)\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                # Stop training when `val_loss` is no longer improving\n",
    "                monitor=\"val_loss\",\n",
    "                # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "                min_delta=1e-2,\n",
    "                # \"no longer improving\" being further defined as \"for at least 5 epochs\"\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        super().__init__(tokenizer, model)\n",
    "     \n",
    "    def __create_model(self):\n",
    "        inputs = layers.Input(shape=(self.max_length-1,), dtype=tf.int32)\n",
    "        embedding_layer = TokenAndPositionEmbedding(self.max_length-1, self.vocab_size, self.embedding_dim)\n",
    "        x = embedding_layer(inputs)\n",
    "        transformerLayers = []\n",
    "        outputs_x = []\n",
    "        for i in range(self.n_transformer_blocks):\n",
    "            transformerLayers.append(TransformerBlock(self.embedding_dim, self.n_heads, self.ff_dim))\n",
    "\n",
    "        for i in range(self.n_transformer_blocks):\n",
    "            x = transformerLayers[i](x)\n",
    "            outputs_x.append(x)\n",
    "        # transformer_block = TransformerBlock(self.embedding_dim, self.n_heads, self.ff_dim)\n",
    "        # x = transformer_block(x)\n",
    "\n",
    "        outputs = layers.Dense(self.vocab_size)(outputs_x[self.n_transformer_blocks-1])\n",
    "        # outputs = layers.Dense(self.vocab_size)(x)\n",
    "        model = keras.Model(inputs=inputs, outputs=[outputs, *outputs_x])\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_path(path, optimizer, loss):\n",
    "        path = Path(path)\n",
    "        # Load tokenizer\n",
    "        tokenizer = Tokenizer.from_file(str(path / \"tokenizer.json\"))\n",
    "        # Load model\n",
    "        with open(path / \"model_config.json\", \"r\") as f:\n",
    "            model_config = json.load(f)\n",
    "            \n",
    "        min_model = MiniatureGPTModel(\n",
    "            path,\n",
    "            **model_config,\n",
    "            tokenizer=tokenizer,\n",
    "            optimizer=optimizer, \n",
    "            loss=loss\n",
    "        )\n",
    "        \n",
    "        loaded_model = tf.keras.models.load_model(str(path), custom_objects={\"_loss\": _loss})\n",
    "        min_model.model = loaded_model\n",
    "        \n",
    "        return min_model\n",
    "        \n",
    "    def save(self):\n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save(str(self.out_path / \"tokenizer.json\"), pretty=True)\n",
    "        # Save model\n",
    "        self.model.save(str(self.out_path))\n",
    "        # Save config.\n",
    "        model_config = {\n",
    "            \"max_length\": self.max_length,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"n_transformer_blocks\": self.n_transformer_blocks,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "        }\n",
    "        with open(self.out_path / \"model_config.json\", \"w\") as file:\n",
    "            json.dump(model_config, file)\n",
    "        \n",
    "    def tokenize(self, method):\n",
    "        \"\"\"\n",
    "        :param method: Code snippet\n",
    "        :returns: Encoded result using the provided tokenizer\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        # encode method and then convert to format that hf models expect\n",
    "        encoding = self.tokenizer.encode(\"<sos>\" + method)\n",
    "        output[\"input_ids\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0\n",
    "        )\n",
    "        output[\"attention_mask\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0\n",
    "        )\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def generate(self, n, top_k=50):\n",
    "        \"\"\"Performs generation process through top-k sampling process\"\"\"\n",
    "         # Converting our start string to numbers (vectorizing)\n",
    "        start_tokens = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        \n",
    "        generated_txt = []\n",
    "        \n",
    "        while len(generated_txt) < n:\n",
    "            gen_sample = self.__sample_top_k(start_tokens, top_k)\n",
    "            generated_txt.append(gen_sample)\n",
    "        \n",
    "        txt_seqs = []\n",
    "        for seq in generated_txt:\n",
    "            txt_seqs.append(self.tokenizer.encode(seq))\n",
    "            \n",
    "        return txt_seqs\n",
    "        \n",
    "    def __sample_top_k(self, start_tokens, top_k=10):\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = [] \n",
    "        gen_sequence = start_tokens[:]\n",
    "        while len(gen_sequence) <= self.max_length-1:\n",
    "            pad_len = (self.max_length-1) - len(gen_sequence)\n",
    "            sample_index = len(gen_sequence) - 1\n",
    "            if pad_len < 0:\n",
    "                x = gen_sequence[:self.max_length-1]\n",
    "                sample_index = self.max_length - 1\n",
    "            elif pad_len > 0:\n",
    "                x = gen_sequence + [0] * pad_len\n",
    "            else:\n",
    "                x = gen_sequence[:]\n",
    "            sample_index = len(gen_sequence)-1\n",
    "            x = np.array([x])\n",
    "            prediction_output = self.model.predict(x)\n",
    "            y = prediction_output[0]\n",
    "            sample_token = self.__sample_from(y[0][sample_index], top_k)\n",
    "            tokens_generated.append(sample_token)\n",
    "            gen_sequence.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "            \n",
    "        return tokens_generated\n",
    "            \n",
    "    def __sample_from(self, logits, top_k):\n",
    "        logits, indices = tf.math.top_k(logits, k=top_k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "    \n",
    "    def get_probs(self, inputs):\n",
    "        #         ids = self.tokenizer.encode(\"<sos>\" + method).ids\n",
    "        #         input_eval = tf.expand_dims(ids, 0)\n",
    "\n",
    "        logits = self.model(inputs[\"input_ids\"])\n",
    "        probs = tf.nn.softmax(logits)  # [0].numpy()\n",
    "\n",
    "        return probs\n",
    "        \n",
    "    def train(self, ds_train, ds_val, epochs):\n",
    "        \"\"\"\n",
    "        Performs training process leveraging keras methods\n",
    "        \"\"\"\n",
    "        self.model.compile(optimizer=self.optimizer, loss=[self.loss, *[None]* self.n_transformer_blocks])\n",
    "        history = self.model.fit(ds_train, epochs=epochs, callbacks=self.callbacks, validation_data=ds_val)\n",
    "        \n",
    "        return history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test Miniature GPT. Keras-based"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Configuration parameters\n",
    "# \n",
    "min_gpt_config_params = {\n",
    "    \"out_path\": \"models/Min-GPT\",\n",
    "    \"max_length\" : max_length,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"ff_dim\": 64,\n",
    "    \"embedding_dim\": 128,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_transformer_blocks\": 1,\n",
    "    \"vocab_size\": tokenizer.get_vocab_size()\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Optimizer, loss definition\n",
    "\n",
    "min_gpt_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "min_gpt_loss = _loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min_gpt_config_params[\"optimizer\"] = min_gpt_optimizer\n",
    "min_gpt_config_params[\"loss\"] = min_gpt_loss\n",
    "min_gpt_config_params[\"tokenizer\"] = tokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min_gpt_model = MiniatureGPTModel(**min_gpt_config_params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_trn_min_time = time.time()\n",
    "min_gpt_history = min_gpt_model.train(dataset, dataset, epochs=TRAIN_EPOCHS)\n",
    "end_trn_min_time = time.time()\n",
    "print(f'Time elapsed: {end_trn_min_time - start_trn_min_time} seconds')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min_gpt_model.model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gen_sequences = min_gpt_model.generate(n=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for seq in gen_sequences:\n",
    "    print('-'*100)\n",
    "    print(f'seq: {tokenizer.decode(seq, skip_special_tokens=False)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min_gpt_model.save()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loaded_min_gpt = MiniatureGPTModel.from_path(\"models/Min-GPT/Miniature-GPT-20210330-200608\", optimizer, min_gpt_loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type(loaded_min_gpt)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "class RNNModel(Model):\n",
    "    _RNN_TYPE = {\n",
    "        \"rnn\": tf.keras.layers.SimpleRNN,\n",
    "        \"gru\": tf.keras.layers.GRU,\n",
    "        \"lstm\": tf.keras.layers.LSTM,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type,\n",
    "        n_layers,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        rnn_units,\n",
    "        batch_size,\n",
    "        out_path,\n",
    "        tokenizer,\n",
    "    ):\n",
    "        self.rnn_type = rnn_type\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rnn_units = rnn_units\n",
    "\n",
    "        self.config_name = f\"{rnn_type}_layers{n_layers}_vocab{vocab_size}_embed{embedding_dim}_units{rnn_units}\"\n",
    "        self.out_path = Path(out_path) / self.config_name\n",
    "        self.out_path.mkdir(exist_ok=True)\n",
    "        tensorboard_path = self.out_path / \"tensorboard_logs\"\n",
    "        tensorboard_path.mkdir(exist_ok=True)\n",
    "        self.callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=self.out_path / \"ckpt_{epoch}\", save_weights_only=True\n",
    "            ),\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=str(tensorboard_path),\n",
    "                histogram_freq=0,  # How often to log histogram visualizations\n",
    "                embeddings_freq=0,  # How often to log embedding visualizations\n",
    "                update_freq=\"epoch\",\n",
    "            ),  # How often to write logs (default: once per epoch)\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                # Stop training when `val_loss` is no longer improving\n",
    "                monitor=\"val_loss\",\n",
    "                # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "                min_delta=1e-2,\n",
    "                # \"no longer improving\" being further defined as \"for at least 5 epochs\"\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        layer = RNNModel._RNN_TYPE[rnn_type]\n",
    "        rnn_layers = [\n",
    "            layer(\n",
    "                rnn_units,\n",
    "                return_sequences=True,\n",
    "                recurrent_initializer=\"glorot_uniform\",\n",
    "                # following BigCode != Big Vocab Paper\n",
    "                dropout=0.5,\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=vocab_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    mask_zero=True,  # Zero cannot be used in the vocabulary\n",
    "                ),\n",
    "            ]\n",
    "            + rnn_layers\n",
    "            + [\n",
    "                tf.keras.layers.Dense(vocab_size),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        super().__init__(tokenizer, model)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_path(path):\n",
    "        path = Path(path)\n",
    "\n",
    "        tokenizer = Tokenizer.from_file(str(path / \"tokenizer.json\"))\n",
    "        with open(path / \"model_config.json\", \"r\") as f:\n",
    "            model_config = json.load(f)\n",
    "\n",
    "        model = RNNModel(\n",
    "            model_config[\"rnn_type\"],\n",
    "            model_config[\"n_layers\"],\n",
    "            model_config[\"vocab_size\"],\n",
    "            model_config[\"embedding_dim\"],\n",
    "            model_config[\"rnn_units\"],\n",
    "            1,\n",
    "            path,\n",
    "            tokenizer,\n",
    "        )\n",
    "        model.model = tf.keras.models.load_model(\n",
    "            str(path), custom_objects={\"_loss\": _loss}\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_probs(self, inputs):\n",
    "        #         ids = self.tokenizer.encode(\"<sos>\" + method).ids\n",
    "        #         input_eval = tf.expand_dims(ids, 0)\n",
    "\n",
    "        logits = self.model(inputs)\n",
    "        probs = tf.nn.softmax(logits)  # [0].numpy()\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def generate(self, n, temperature=1.0):\n",
    "        # Converting our start string to numbers (vectorizing)\n",
    "        text_generated = [self.tokenizer.encode(\"<sos>\").ids[0]]\n",
    "        input_eval = tf.expand_dims(text_generated, 0)\n",
    "\n",
    "        # Here batch size == 1\n",
    "        self.model.reset_states()\n",
    "        for i in range(n):\n",
    "            predictions = self.model(input_eval)\n",
    "            # remove the batch dimension\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "            # using a categorical distribution to predict the character\n",
    "            # returned by the model\n",
    "            predictions = predictions / temperature\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[\n",
    "                -1, 0\n",
    "            ].numpy()\n",
    "\n",
    "            text_generated.append(predicted_id)\n",
    "            # Pass the predicted character as the next input to the model\n",
    "            # along with the previous hidden state\n",
    "            input_eval = tf.expand_dims(text_generated, 0)\n",
    "\n",
    "        return self.tokenizer.decode(text_generated, skip_special_tokens=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.tokenizer.save(str(self.out_path / \"tokenizer.json\"), pretty=True)\n",
    "        self.model.save(str(self.out_path))\n",
    "        model_config = {\n",
    "            \"rnn_type\": self.rnn_type,\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"rnn_units\": self.rnn_units,\n",
    "        }\n",
    "        with open(self.out_path / \"model_config.json\", \"w\") as f:\n",
    "            json.dump(model_config, f)\n",
    "\n",
    "    def tokenize(self, method):\n",
    "        #         ids = self.tokenizer.encode(\"<sos>\" + method).ids\n",
    "        #         inputs = tf.expand_dims(ids, 0)\n",
    "        output = {}\n",
    "        # encode method and then convert to format that hf models expect\n",
    "        encoding = self.tokenizer.encode(\"<sos>\" + method)\n",
    "        output[\"input_ids\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.ids, dtype=tf.int32), 0\n",
    "        )\n",
    "        output[\"attention_mask\"] = tf.expand_dims(\n",
    "            tf.convert_to_tensor(encoding.attention_mask, dtype=tf.int32), 0\n",
    "        )\n",
    "        return output  # self.tokenizer(method, return_tensors=\"tf\")\n",
    "\n",
    "    # TODO add tensorboard call back for easy visualization\n",
    "    def train(self, ds_trn, ds_val, epochs):\n",
    "        self.model.compile(optimizer=\"adam\", loss=_loss)\n",
    "        history = self.model.fit(\n",
    "            ds_trn, epochs=epochs, callbacks=self.callbacks, validation_data=ds_val\n",
    "        )\n",
    "\n",
    "        return history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RNN models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_path = Path(\"/tmp/data/codesearchnet\")\n",
    "df_trn = pd.read_json(\n",
    "    data_path / \"codesearchnet_java\" / \"train.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")[:50]\n",
    "ds_trn = convert_df_to_tfds(df_trn, tokenizer, max_length, batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tok_trn = tokenizer.encode_batch(df_trn.code.values)\n",
    "hf_fast_tok_form = {\n",
    "    \"input_ids\": [x.ids for x in tok_trn],\n",
    "    \"attention_mask\": [x.attention_mask for x in tok_trn],\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (dict(train_encodings), train_labels)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer.encode(\"hi\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "metadata": {},
     "execution_count": null
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer.encode_batch()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tok_path = Path(\"/tmp/models/tokenizer.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import GPT2TokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "new_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_tokenizer([\"Hi there\", \"WasaaP\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [[17250, 612], [16973, 7252, 47]], 'attention_mask': [[1, 1], [1, 1, 1]]}"
      ]
     },
     "metadata": {},
     "execution_count": null
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "GPT2TokenizerFast()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'vocab_file' and 'merges_file'",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-02b17b568bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'vocab_file' and 'merges_file'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_tokenizer.backend_tokenizer = tokenizer"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-da4f2361f6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=model.compute_loss\n",
    ")  # can also use any keras loss fn\n",
    "# model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "class WildModel(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "    ):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=50256)\n",
    "        super().__init__(tokenizer, model)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_path(path):\n",
    "        pass\n",
    "\n",
    "    def get_probs(self, inputs):\n",
    "        # inputs = torch.from_numpy(inputs)\n",
    "        logits = self.model(**inputs).logits\n",
    "        probs = F.softmax(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def generate(self, n, temperature=1.0):\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "    def batch_tokenize(self, batch):\n",
    "        return self.tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    def tokenize(self, method):\n",
    "        pass\n",
    "\n",
    "    # TODO add tensorboard call back for easy visualization\n",
    "    def train(self, ds_trn, ds_val, epochs):\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PARAM_COUNT = 124_772\n",
    "\n",
    "rnn_type = \"gru\"\n",
    "n_layers = 1\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 128\n",
    "rnn_units = 128\n",
    "batch_size = 1\n",
    "out_path = \"/tmp\"\n",
    "gru = RNNModel(\n",
    "    rnn_type,\n",
    "    n_layers,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    rnn_units,\n",
    "    batch_size,\n",
    "    out_path,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "assert PARAM_COUNT == gru.model.count_params()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gpt2_configuration = GPT2Config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TFLM"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gpt2_model = TransformerModel('gpt2_model', , tokenizer, optimizer, _loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transformer_history = gpt2_model.train(dataset, dataset, EPOCHS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gpt2_model.generate(10, 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "EPOCHS = 1\n",
    "chkpt_path = Path(out_path) / (\n",
    "    f\"{rnn_type}_layers{n_layers}_vocab{vocab_size}_embed{embedding_dim}_units{rnn_units}\"\n",
    ")\n",
    "history = gru.train(dataset, dataset, EPOCHS)\n",
    "\n",
    "assert chkpt_path.exists()\n",
    "assert EPOCHS == len(list(chkpt_path.glob(\"*.index\")))\n",
    "assert EPOCHS == len(history.history[\"loss\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# How do I test this?\n",
    "text = \"test\"\n",
    "text_generated = tokenizer.encode(\"<sos>\" + text).ids\n",
    "input_eval = tf.expand_dims(text_generated, 0)\n",
    "\n",
    "logits = gru.model(input_eval)[0].numpy()\n",
    "inputs = gru.tokenize(text)\n",
    "probs = gru.get_probs(inputs)[0].numpy()\n",
    "\n",
    "for i in range(len(probs)):\n",
    "    assert np.isclose(1.0, probs[i].sum())\n",
    "    assert np.argmax(logits[i]) == np.argmax(probs[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Add test case for earlystopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # this test will break sometimes due to encoding adding space prefixes to some\n",
    "# # tokens when add_prefix_space=False it should be fine :/\n",
    "# NUM_TOKENS = 10\n",
    "# text = gru.generate(NUM_TOKENS)\n",
    "# tokenizer.no_padding()\n",
    "# ids = tokenizer.encode(text).ids\n",
    "# tokenizer.enable_padding(length=max_length)\n",
    "\n",
    "# # -1 for the <sos> token that's always prepended\n",
    "# assert NUM_TOKENS == len(ids) - 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gru.save()\n",
    "\n",
    "assert (gru.out_path / \"assets\").exists()\n",
    "assert (gru.out_path / \"model_config.json\").exists()\n",
    "assert (gru.out_path / \"saved_model.pb\").exists()\n",
    "assert (gru.out_path / \"tokenizer.json\").exists()\n",
    "assert (gru.out_path / \"variables\").exists()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loaded_gru = RNNModel.from_path(str(gru.out_path))\n",
    "\n",
    "assert gru.tokenizer.get_vocab() == loaded_gru.tokenizer.get_vocab()\n",
    "assert gru.model.count_params() == loaded_gru.model.count_params()\n",
    "assert gru.model.evaluate(dataset, verbose=2) == loaded_gru.model.evaluate(\n",
    "    dataset, verbose=2\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "# Experiment 0.0.0\n",
    "VANILLA_CONFIG = {\n",
    "    \"rnn_type\": \"rnn\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.0.0\n",
    "GRU_CONFIG_1 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.1.0\n",
    "GRU_CONFIG_2 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 2,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.1.1\n",
    "GRU_CONFIG_3 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 3,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 1_024,\n",
    "}\n",
    "\n",
    "# Experiment 1.2.0\n",
    "GRU_CONFIG_4 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 512,\n",
    "}\n",
    "\n",
    "# Experiment 1.2.1\n",
    "GRU_CONFIG_5 = {\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"n_layers\": 1,\n",
    "    \"embedding_dim\": 256,\n",
    "    \"rnn_units\": 2_048,\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "def train(\n",
    "    data_path, out_path, epochs=64, max_length=300, batch_size=64, configs=[], n=None\n",
    "):\n",
    "    \"\"\"Function for training models related to the library.\"\"\"\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Load in the datasets\n",
    "\n",
    "    # Train BPE tokenizer\n",
    "    # Check if the path where the tokenizer is to be saved is not empty\n",
    "    # if it is not empty then just load the tokenizer there.\n",
    "    if (out_path / \"tokenizer.json\").exists():\n",
    "        logging.info(f\"Loading tokenizer from {str(out_path / 'tokenizer.json')}.\")\n",
    "        tokenizer = Tokenizer.from_file(str(out_path / \"tokenizer.json\"))\n",
    "    else:\n",
    "        logging.info(\n",
    "            f\"Training new tokenizer and saving to {str(out_path / 'tokenizer.json')}.\"\n",
    "        )\n",
    "        df_bpe = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"bpe.jsonl\", orient=\"records\", lines=True\n",
    "        )[:n]\n",
    "        tokenizer = train_tokenizer(df_bpe, java_special_tokens, max_length)\n",
    "        tokenizer.save(str(out_path / \"tokenizer.json\"), pretty=True)\n",
    "        del df_bpe\n",
    "\n",
    "    # Tokenize the dataset and convert it to tfds.\n",
    "    tfds_trn_path = (\n",
    "        data_path / \"codesearchnet_java\" / f\"tfds_trn_{max_length}len_{batch_size}bs\"\n",
    "    )\n",
    "    if tfds_trn_path.exists():\n",
    "        ds_trn = tf.data.experimental.load(\n",
    "            str(tfds_trn_path),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_trn = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"train.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )[:n]\n",
    "        ds_trn = convert_df_to_tfds(df_trn, tokenizer, max_length, batch_size)\n",
    "        tfds_trn_path.mkdir(exist_ok=True)\n",
    "        tf.data.experimental.save(ds_trn, str(tfds_trn_path))\n",
    "        del df_trn\n",
    "\n",
    "    tfds_val_path = (\n",
    "        data_path / \"codesearchnet_java\" / f\"tfds_val_{max_length}len_{batch_size}bs\"\n",
    "    )\n",
    "    if tfds_val_path.exists():\n",
    "        ds_val = tf.data.experimental.load(\n",
    "            str(tfds_val_path),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_val = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"valid.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )[:n]\n",
    "        ds_val = convert_df_to_tfds(df_val, tokenizer, max_length, batch_size)\n",
    "        tfds_val_path.mkdir(exist_ok=True)\n",
    "        tf.data.experimental.save(ds_val, str(tfds_val_path))\n",
    "        del df_val\n",
    "\n",
    "    logging.info(\"Starting the training of all RNN based models.\")\n",
    "    # Train RNN based models\n",
    "    for config in configs:\n",
    "        rnn_model = RNNModel(\n",
    "            config[\"rnn_type\"],\n",
    "            config[\"n_layers\"],\n",
    "            tokenizer.get_vocab_size(),\n",
    "            config[\"embedding_dim\"],\n",
    "            config[\"rnn_units\"],\n",
    "            batch_size,\n",
    "            str(out_path),\n",
    "            tokenizer,\n",
    "        )\n",
    "        rnn_model.train(ds_trn, ds_val, epochs)\n",
    "        rnn_model.save()\n",
    "\n",
    "    logging.info(\"Starting the training of all Transformer based models.\")\n",
    "    # Train Transformer models\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_path = Path(\"/tmp/data/codesearchnet\")\n",
    "models_path = Path(\"/tmp/models\")\n",
    "rnn_path = models_path / \"rnn_layers1_vocab10000_embed256_units1024\"\n",
    "EPOCHS = 2\n",
    "MAX_LEN = 100\n",
    "BS = 8\n",
    "N = 100\n",
    "_RNN_CONFIGs = [VANILLA_CONFIG]\n",
    "\n",
    "train(\n",
    "    data_path=data_path,\n",
    "    out_path=models_path,\n",
    "    epochs=EPOCHS,\n",
    "    max_length=MAX_LEN,\n",
    "    batch_size=BS,\n",
    "    configs=_RNN_CONFIGs,\n",
    "    n=N,\n",
    ")\n",
    "\n",
    "assert (models_path / \"tokenizer.json\").exists()\n",
    "assert rnn_path.exists()\n",
    "assert (rnn_path / \"assets\").exists()\n",
    "assert (rnn_path / \"model_config.json\").exists()\n",
    "assert (rnn_path / \"saved_model.pb\").exists()\n",
    "assert (rnn_path / \"variables\").exists()\n",
    "assert EPOCHS == len(list(rnn_path.glob(\"*.index\")))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "def _get_tkzr_ds(out_path: Path, data_path: Path,\n",
    "                 max_length: int, batch_size: int,\n",
    "                 n: Optional[int]=None) -> tuple:\n",
    "    \"\"\"\n",
    "    Function to handle the configuration of tokenizer and related datasets for training\n",
    "    :param out_path: Path referencing the output directory\n",
    "    :param max_length: Maximum length of sequences allowed by the models\n",
    "    :batch_size: Batch size to  config. datasets\n",
    "    \n",
    "    :return: Tuple (tokenizer, dataset_train, dataset_validation)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in the datasets\n",
    "    # Train BPE tokenizer\n",
    "    # Check if the path where the tokenizer is to be saved is not empty\n",
    "    # if it is not empty then just load the tokenizer there.\n",
    "    \n",
    "    if (out_path / \"tokenizer.json\").exists():\n",
    "        logging.info(f\"Loading tokenizer from {str(out_path / 'tokenizer.json')}.\")\n",
    "        tokenizer = Tokenizer.from_file(str(out_path / \"tokenizer.json\"))\n",
    "    else:\n",
    "        logging.info(\n",
    "            f\"Training new tokenizer and saving to {str(out_path / 'tokenizer.json')}.\"\n",
    "        )\n",
    "        df_bpe = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"bpe.jsonl\", orient=\"records\", lines=True\n",
    "        )[:n]\n",
    "        tokenizer = train_tokenizer(df_bpe, java_special_tokens, max_length)\n",
    "        tokenizer.save(str(out_path / \"tokenizer.json\"), pretty=True)\n",
    "        del df_bpe\n",
    "\n",
    "    # Tokenize the dataset and convert it to tfds.\n",
    "    tfds_trn_path = (\n",
    "        data_path / \"codesearchnet_java\" / f\"tfds_trn_{max_length}len_{batch_size}bs\"\n",
    "    )\n",
    "    if tfds_trn_path.exists():\n",
    "        ds_trn = tf.data.experimental.load(\n",
    "            str(tfds_trn_path),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_trn = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"train.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )[:n]\n",
    "        ds_trn = convert_df_to_tfds(df_trn, tokenizer, max_length, batch_size)\n",
    "        tfds_trn_path.mkdir(exist_ok=True)\n",
    "        tf.data.experimental.save(ds_trn, str(tfds_trn_path))\n",
    "        del df_trn\n",
    "\n",
    "    tfds_val_path = (\n",
    "        data_path / \"codesearchnet_java\" / f\"tfds_val_{max_length}len_{batch_size}bs\"\n",
    "    )\n",
    "    if tfds_val_path.exists():\n",
    "        ds_val = tf.data.experimental.load(\n",
    "            str(tfds_val_path),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(batch_size, max_length - 1), dtype=tf.int32),\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        df_val = pd.read_json(\n",
    "            data_path / \"codesearchnet_java\" / \"valid.jsonl\",\n",
    "            orient=\"records\",\n",
    "            lines=True,\n",
    "        )[:n]\n",
    "        ds_val = convert_df_to_tfds(df_val, tokenizer, max_length, batch_size)\n",
    "        tfds_val_path.mkdir(exist_ok=True)\n",
    "        tf.data.experimental.save(ds_val, str(tfds_val_path))\n",
    "        del df_val\n",
    "        \n",
    "    return tokenizer, ds_trn, ds_val"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#export\n",
    "\n",
    "#references:\n",
    "#GradientAccumulator class: https://huggingface.co/transformers/_modules/transformers/optimization_tf.html#GradientAccumulator\n",
    "#use in TFTrainer: https://huggingface.co/transformers/_modules/transformers/trainer_tf.html#TFTrainer\n",
    "\n",
    "def get_accumulation_optimizer(optimizer_class, steps = 1, **kwargs):\n",
    "    \"\"\"\n",
    "    Creates a custom optimizer that adds gradient accumulation functionality to a tf.keras optimizer\n",
    "    :param optimizer_class: Class of optimizer to add gradient accumulation to. The returned optimizer will be a subclass of this class\n",
    "    :param steps: Number of gradient accumulation steps, i.e. the number of batches over which to accumulate gradients\n",
    "    :param kwargs: Any other keyword arguments to be passed to the optimizer_class constructor\n",
    "    :returns: AccumulationOptimizer instance that is a subclass of optimizer_class with gradient accumulation\n",
    "    \"\"\"\n",
    "    \n",
    "    class AccumulationOptimizer(optimizer_class):\n",
    "        def __init__(self, steps, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.batch = 0\n",
    "            self.steps = steps\n",
    "            self.accumulator = GradientAccumulator()\n",
    "            self.accumulator.reset()\n",
    "            self.count = 0\n",
    "            \n",
    "        def apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True):\n",
    "            self.count +=1\n",
    "            return_value = None\n",
    "            if not hasattr(self, 'vars'):\n",
    "                self.vars = [gradvar[1] for gradvar in grads_and_vars]\n",
    "            \n",
    "            self.batch += 1\n",
    "            self.accumulator([gradvar[0] for gradvar in grads_and_vars])\n",
    "            \n",
    "            if self.batch == self.steps:\n",
    "                return_value = self.apply_accumulated_gradients(name, experimental_aggregate_gradients)\n",
    "            return return_value\n",
    "            \n",
    "            \n",
    "        def apply_accumulated_gradients(self, name=None, experimental_aggregate_gradients=True):\n",
    "            if self.batch != 0:\n",
    "                return_value = super().apply_gradients(zip(self.accumulator.gradients, self.vars),\n",
    "                                                       name, experimental_aggregate_gradients)\n",
    "                self.accumulator.reset()\n",
    "                self.batch = 0\n",
    "                return return_value\n",
    "            \n",
    "    return AccumulationOptimizer(steps, **kwargs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#export\n",
    "\n",
    "class AccumulationCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback for use with an AccumulationOptimizer.\n",
    "    Applies excess gradients at the end of an epoch when the number of batches is not divisible by the number of accumulation steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logging.debug(self.optimizer.count)\n",
    "        self.optimizer.apply_accumulated_gradients()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "def train_tfr_hf_model(hf_model, config_class, config_dict,\n",
    "                       data_path, out_path, optimizer, loss,\n",
    "                       epochs=64, max_length=300,\n",
    "                       batch_size=64, n=None, pretrained_model=None, gradient_accumulation = False):\n",
    "    \"\"\"\n",
    "    Function to train a Transformer model according to the provided params\n",
    "    :param hf_model: Class of the HF model to implement the Transformer\n",
    "    :param config_class: HuggingFace class of the config. to apply for the model\n",
    "    :param config_dict: Dictionary with the params to config. the model (ex: n_layers)\n",
    "    :param data_path: Path corresponding to the data to be used\n",
    "    :param out_path: Path corresponding to the path to save model-related data\n",
    "    :param optimizer:\n",
    "    :param loss: \n",
    "    :param epochs: Number of epochs for the training process\n",
    "    :param max_length: Maximum length of the sequences processed by the model\n",
    "    :param batch_size: Batch size to use\n",
    "    :param n: Number of training points to take (entire ds taken by default)\n",
    "    :param pretrained_model: Name/Path of the pretrained model to load\n",
    "    :param gradient_accumulation: Boolean, set to True if using gradient accumulation with an AccumulationOptimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    out_path.mkdir(exist_ok=True)\n",
    "\n",
    "    tokenizer, ds_trn, ds_val = _get_tkzr_ds(out_path, data_path, max_length, batch_size, n)\n",
    "    \n",
    "    # Implement custom HF model\n",
    "    name_sufix = None\n",
    "    if pretrained_model is None:\n",
    "        logging.info(\"Implementing model.\")\n",
    "        custom_config = config_class(\n",
    "            vocab_size = tokenizer.get_vocab_size(),\n",
    "            bos_token_id = tokenizer.token_to_id(\"<sos>\"),\n",
    "            eos_token_id = tokenizer.token_to_id(\"<eos>\"),\n",
    "            pad_token_id = tokenizer.token_to_id(\"<pad>\"),\n",
    "            **config_dict\n",
    "        )\n",
    "        logging.info(f\"Used configuration: {custom_config}\")\n",
    "        hf_model_impl = hf_model(custom_config)\n",
    "    else:\n",
    "        logging.info(f\"Loading pretrained model from: {pretrained_model}\")\n",
    "        name_sufix = 'pretrained'\n",
    "        hf_model_impl = hf_model.from_pretrained(pretrained_model)\n",
    "        \n",
    "    tfr_model = TransformerHFModel(str(out_path), hf_model_impl, tokenizer, optimizer, loss, name_sufix=name_sufix)\n",
    "    if gradient_accumulation:\n",
    "        tfr_model.callbacks.append(AccumulationCallback(optimizer))\n",
    "    \n",
    "    logging.info(\"Starting the training of the Transformer model.\")\n",
    "    start_trn_time = time.time()\n",
    "    tfr_model.train(ds_trn, ds_val, epochs)\n",
    "    end_trn_time = time.time()\n",
    "    \n",
    "    logging.info(f'Finished training, Time elapsed: {end_trn_time - start_trn_time} seconds.')\n",
    "    \n",
    "    tfr_model.save()\n",
    "    logging.info(f'Model saved.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "def train_tfr_keras_model(config_dict, optimizer, loss, out_path,\n",
    "                          data_path, epochs=64, max_length=300,\n",
    "                          batch_size=64, n=None):\n",
    "    \"\"\"\n",
    "    Function to train Transformer models based on the miniature implementation provided\n",
    "    by Keras\n",
    "    \n",
    "    :param config_dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    tokenizer, ds_trn, ds_val = _get_tkzr_ds(out_path, data_path, max_length, batch_size, n)\n",
    "    \n",
    "    config_dict['out_path'] = str(out_path)\n",
    "    config_dict['max_length'] = max_length\n",
    "    config_dict['batch_size'] = batch_size\n",
    "    config_dict['vocab_size'] = tokenizer.get_vocab_size()\n",
    "    config_dict['optimizer'] = optimizer\n",
    "    config_dict['loss'] = loss\n",
    "    config_dict['tokenizer'] = tokenizer\n",
    "    \n",
    "    min_gpt_model = MiniatureGPTModel(**config_dict)\n",
    "    \n",
    "    logging.info(\"Starting the training of the Transformer model.\")\n",
    "    start_trn_time = time.time()\n",
    "    min_gpt_model.train(ds_trn, ds_val, epochs)\n",
    "    end_trn_time = time.time()\n",
    "    \n",
    "    logging.info(f'Finished training, Time elapsed: {end_trn_time - start_trn_time} seconds.')\n",
    "    \n",
    "    min_gpt_model.save()\n",
    "    logging.info(f'Model saved.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sampling (Generation)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although the implementation of models uses a common interface (class Model), the details for the generation (and training) process differ, specially for the models using HuggingFace underneath. For that reason, multiple methods are provided to perform sampling.\n",
    "\n",
    "The storage of sample sets follows a convetion similar to the one used for storing model-related data: Each models has its own directory and its related samples are stored there, with a convetion as seen in the <i>_save_samples</i> method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "def _save_samples(df: pd.DataFrame, out_path: str, config_name: str, file_format:Optional[str]=\"jsonl\"):\n",
    "    \"\"\"\n",
    "    Function to store samples for a specific model to a jsonl file.\n",
    "    :param df: Pandas Dataframe containing samples\n",
    "    :param out_path: Directory path to store the csv files\n",
    "    :param config_name: Model config. for handle file storage naming\n",
    "    \n",
    "    \"\"\"\n",
    "    out_path = Path(out_path)\n",
    "    out_path = out_path / config_name\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "    now = datetime.datetime.now()\n",
    "    ts = datetime.datetime.timestamp(now)\n",
    "    \n",
    "    n = len(df)\n",
    "    full_path = out_path/f\"{n}-samples_{ts}.{file_format}\"\n",
    "    logging.info(f\"Saving samples to {full_path}\")\n",
    "    if file_format == \"jsonl\":\n",
    "        df.to_json(full_path, orient=\"records\", lines=True)\n",
    "    else:\n",
    "        df.to_csv(full_path, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "def perform_hf_tfr_sampling(gen_model: TransformerHFModel, n: int,\n",
    "                            max_length: int, special_toks: Dict[str, str],\n",
    "                            out_path: Optional[str]=None,\n",
    "                            decoding_strategy: Optional[str]=None,\n",
    "                            file_format: Optional[str]=\"jsonl\",\n",
    "                            max_allowed: Optional[int]=1000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to perform the sampling (generation process) for a given (hf) model and params.\n",
    "    :param gen_model: Model instance of a loaded model to be used for generation\n",
    "    :param n: Number of samples to generate\n",
    "    :param special_toks: Dict containing the special tokens to replace\n",
    "    :param out_path: Path to store the generated samples\n",
    "    :param decoding_trategy:\n",
    "    :param max_allowed: Max. number of samples to generate simultaneously (using GPU)\n",
    "    \n",
    "    :return: Pandas DataFrame containing the number of generated samples\n",
    "    \"\"\"\n",
    "    logging.info('Starting sampling process')\n",
    "    generated_seqs = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if n <= max_allowed:\n",
    "        generated_seqs = gen_model.generate(n=n, max_length=max_length)\n",
    "    else:\n",
    "        res = n % max_allowed\n",
    "        n_iterations = int(n/max_allowed)\n",
    "        for i in range(n_iterations):\n",
    "            seqs = gen_model.generate(n=max_allowed, max_length=max_length)\n",
    "            generated_seqs += seqs\n",
    "        \n",
    "        if res > 0:\n",
    "            seqs = gen_model.generate(n=res, max_length=max_length)\n",
    "            generated_seqs += seqs\n",
    "    \n",
    "    gen_df = pd.DataFrame(generated_seqs, columns=[\"code\"])\n",
    "    \n",
    "    # Clean \"raw\" generated data\n",
    "    clean_df = replace_spec_toks_to_original(gen_df, special_toks)\n",
    "    \n",
    "    if out_path is not None:\n",
    "        _save_samples(clean_df, out_path, gen_model.config_name, file_format)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    logging.info(f\"Time elapsed sampling: {end_time - start_time} seconds.\")\n",
    "    return clean_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "def perform_min_tfr_model_sampling(gen_model: MiniatureGPTModel, n: int,\n",
    "                                   max_length: int, special_toks: Dict[str, str],\n",
    "                                   out_path: Optional[str]=None,\n",
    "                                   file_format: Optional[str]=\"jsonl\",\n",
    "                                   top_k: Optional[float]=100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to perform sampling (generation process) for transformers (keras implementation)    \n",
    "    models.\n",
    "    :param:\n",
    "    \n",
    "    :return: pd DataFrame containing generated code snippets\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    gen_samples = gen_model.generate(n, top_k)\n",
    "    gen_df = pd.DataFrame(gen_samples, columns=[\"code\"])\n",
    "    \n",
    "    # Clean \"raw\" generated data\n",
    "    clean_df = replace_spec_toks_to_original(gen_df, special_toks)\n",
    "    \n",
    "    if out_path is not None:\n",
    "        _save_samples(clean_df, out_path, gen_model.model_config, file_format)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    logging.info(f\"Time elapsed sampling: {end_time - start_time} seconds.\")\n",
    "    \n",
    "    return clean_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# export\n",
    "\n",
    "def perform_rnn_sampling(gen_model: RNNModel, n: int, max_length: int,\n",
    "                         special_toks: Dict[str, str], temperature: Optional[float]=1.0,\n",
    "                         out_path: Optional[str]=None, file_format: Optional[str]=\"jsonl\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to perform sampling (generation) process for RNN models\n",
    "    \n",
    "    :return: pandas DataFrame containing generated code snippets\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    samples = []\n",
    "    while(len(samples) < n):\n",
    "        sample = gen_model.generate(max_length, temperature)\n",
    "        samples.append(sample)\n",
    "        \n",
    "    gen_df = pd.DataFrame(samples, columns=[\"code\"])\n",
    "    \n",
    "    # Clean \"raw\" generated data\n",
    "    clean_df = replace_spec_toks_to_original(gen_df, special_toks)\n",
    "    \n",
    "    if out_path is not None:\n",
    "        _save_samples(clean_df, out_path, gen_model.config_name, file_format)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    logging.info(f\"Time elapsed sampling: {end_time - start_time} seconds.\")\n",
    "    \n",
    "    return clean_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 00_data.core.ipynb.\n",
      "Converted 00_data.covariates.ipynb.\n",
      "Converted 00_data.eda.ipynb.\n",
      "Converted 01_data.transforms.ipynb.\n",
      "Converted 01_model.gru.ipynb.\n",
      "Converted 02_model.core.ipynb.\n",
      "Converted 04_evaluation.causal.ipynb.\n",
      "Converted 04_evaluation.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}