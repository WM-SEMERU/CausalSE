{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> This model contains all the necessary functionality for managing data. @Nathan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import gdown\n",
    "import icodegen\n",
    "import io\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# from datasets import load_dataset\n",
    "from icodegen.data.transforms import java_comment_remover, transform_df\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from subprocess import CalledProcessError, check_output\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from typing import Dict, Optional\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "URLs = {\n",
    "    \"bigclonebenchmark\": \"https://drive.google.com/uc?id=1-AergjO9SGUDI3v8cj6UYisNZ1aJzxOx\",\n",
    "    \"bigclonebenchmark_lg\": \"https://drive.google.com/uc?id=1-4LPiiKGR5Zmg-TLqZEkRbRIdg7UlJQb\",\n",
    "    \"bigclonebenchmark_sm\": \"https://drive.google.com/uc?id=1FCq0lSs4oqc3jpSoucsHlRqjmbVwdRQ9\",\n",
    "    \"bug_fix_pairs\": \"https://drive.google.com/uc?id=1XEhnsQ3Uy6SnFz349I0Iu9lz4ggAaiQp\",\n",
    "    \"codesearchnet_java\": \"https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _download_data(out_path):\n",
    "    \"\"\"\n",
    "    Function for downloading all the data to reproduce our study.\n",
    "    \"\"\"\n",
    "    # Download bigclonebenchmark_lg and bigclonebenchmark_sm\n",
    "    logging.info(\"Downloading BigCloneBenchmark datasets.\")\n",
    "    bigclone_path = out_path / \"bigclonebenchmark\"\n",
    "    bigclone_path.mkdir(parents=True, exist_ok=True)\n",
    "    gdown.cached_download(\n",
    "        URLs[\"bigclonebenchmark\"],\n",
    "        str(bigclone_path / \"bigclonebenchmark.zip\"),\n",
    "        postprocess=gdown.extractall,\n",
    "    )\n",
    "\n",
    "    # Download Bug Fix Pairs\n",
    "    logging.info(\"Downloading and extracting Bug Fix Pairs dataset.\")\n",
    "    bugfix_path = out_path / \"bug_fix\"\n",
    "    bugfix_path.mkdir(parents=True, exist_ok=True)\n",
    "    gdown.cached_download(\n",
    "        URLs[\"bug_fix_pairs\"],\n",
    "        str(bugfix_path / \"bug_fix_pairs.zip\"),\n",
    "        postprocess=gdown.extractall,\n",
    "    )\n",
    "    with zipfile.ZipFile(\n",
    "        str(bugfix_path / \"datasets\" / \"50-100\" / \"source_code.zip\"), \"r\"\n",
    "    ) as zip_ref:\n",
    "        zip_ref.extractall(bugfix_path)\n",
    "\n",
    "    # from https://stackoverflow.com/a/14260592/5768407 by users\n",
    "    # yoavram (https://stackoverflow.com/users/1063612/yoavram) and\n",
    "    # kamran kausar (https://stackoverflow.com/users/3486460/kamran-kausar)\n",
    "    logging.info(\"Downloading and extracting CodeSearchNet Challenge dataset.\")\n",
    "    codesearchnet_path = out_path / \"codesearchnet\"\n",
    "    if not codesearchnet_path.exists():\n",
    "        codesearchnet_path.mkdir(parents=True, exist_ok=True)\n",
    "        r = requests.get(URLs[\"codesearchnet_java\"])\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(codesearchnet_path / \"codesearchnet_java\")\n",
    "    else:\n",
    "        logging.info(f\"File exists: {codesearchnet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading and extracting Bug Fix Pairs dataset.\n",
      "Cached Downloading: /home/jovyan/work/dvc-icodegen/bug_fix/bug_fix_pairs.zip\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1XEhnsQ3Uy6SnFz349I0Iu9lz4ggAaiQp\n",
      "To: /home/jovyan/.cache/gdown/tmp29haq8_g/dl\n",
      "\n",
      "0.00B [00:00, ?B/s]\u001b[A\n",
      "1.05MB [00:00, 8.47MB/s]\u001b[A\n",
      "3.67MB [00:00, 10.4MB/s]\u001b[A\n",
      "6.29MB [00:00, 12.6MB/s]\u001b[A\n",
      "7.86MB [00:00, 13.0MB/s]\u001b[A\n",
      "10.5MB [00:00, 14.7MB/s]\u001b[A\n",
      "12.6MB [00:00, 15.0MB/s]\u001b[A\n",
      "15.7MB [00:00, 17.2MB/s]\u001b[A\n",
      "17.8MB [00:00, 18.1MB/s]\u001b[A\n",
      "20.4MB [00:01, 18.2MB/s]\u001b[A\n",
      "23.6MB [00:01, 20.8MB/s]\u001b[A\n",
      "26.2MB [00:01, 18.2MB/s]\u001b[A\n",
      "28.3MB [00:01, 18.0MB/s]\u001b[A\n",
      "30.4MB [00:01, 18.5MB/s]\u001b[A\n",
      "32.5MB [00:01, 15.4MB/s]\u001b[A\n",
      "35.1MB [00:01, 17.2MB/s]\u001b[A\n",
      "38.3MB [00:02, 19.0MB/s]\u001b[A\n",
      "40.9MB [00:02, 19.9MB/s]\u001b[A\n",
      "43.5MB [00:02, 21.3MB/s]\u001b[A\n",
      "46.1MB [00:02, 21.3MB/s]\u001b[A\n",
      "48.8MB [00:02, 21.5MB/s]\u001b[A\n",
      "51.4MB [00:02, 20.9MB/s]\u001b[A\n",
      "54.5MB [00:02, 22.1MB/s]\u001b[A\n",
      "57.1MB [00:02, 21.5MB/s]\u001b[A\n",
      "59.8MB [00:02, 22.1MB/s]\u001b[A\n",
      "62.5MB [00:03, 20.1MB/s]\u001b[A\n",
      "64.6MB [00:03, 20.3MB/s]\u001b[A\n",
      "67.2MB [00:03, 21.7MB/s]\u001b[A\n",
      "69.7MB [00:03, 18.7MB/s]\u001b[A\n",
      "71.8MB [00:03, 18.3MB/s]\u001b[A\n",
      "73.9MB [00:03, 15.8MB/s]\u001b[A\n",
      "76.0MB [00:03, 16.5MB/s]\u001b[A\n",
      "78.1MB [00:04, 14.2MB/s]\u001b[A\n",
      "81.3MB [00:04, 16.5MB/s]\u001b[A\n",
      "83.4MB [00:04, 15.4MB/s]\u001b[A\n",
      "85.4MB [00:04, 15.9MB/s]\u001b[A\n",
      "87.5MB [00:04, 14.5MB/s]\u001b[A\n",
      "90.1MB [00:04, 16.2MB/s]\u001b[A\n",
      "92.2MB [00:04, 16.2MB/s]\u001b[A\n",
      "94.8MB [00:05, 17.8MB/s]\u001b[A\n",
      "96.9MB [00:05, 13.4MB/s]\u001b[A\n",
      "99.0MB [00:05, 14.2MB/s]\u001b[A\n",
      "101MB [00:05, 15.1MB/s] \u001b[A\n",
      "103MB [00:05, 11.4MB/s]\u001b[A\n",
      "106MB [00:05, 13.3MB/s]\u001b[A\n",
      "108MB [00:06, 14.0MB/s]\u001b[A\n",
      "110MB [00:06, 15.5MB/s]\u001b[A\n",
      "113MB [00:06, 17.7MB/s]\u001b[A\n",
      "INFO:filelock:Lock 140719373942848 acquired on /home/jovyan/.cache/gdown/_dl_lock\n",
      "INFO:filelock:Lock 140719373942848 released on /home/jovyan/.cache/gdown/_dl_lock\n",
      "0.00B [00:20, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"/home/jovyan/work/dvc-icodegen/\")\n",
    "_download_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out to prevent CI/CD from wasting bandwith downloading\n",
    "# data_path = Path(\"/tmp/data\")\n",
    "# _download_data(data_path)\n",
    "\n",
    "# bigclone_path = data_path / \"bigclonebenchmark\"\n",
    "# assert Path(\n",
    "#     bigclone_path / \"bigclone_types\" / \"bigclone_type_1.jsonl\"\n",
    "# ).exists()\n",
    "# assert Path(\n",
    "#     bigclone_path / \"bigclone_types\" / \"bigclone_type_2.jsonl\"\n",
    "# ).exists()\n",
    "# assert Path(\n",
    "#     bigclone_path / \"bigclone_types\" / \"bigclone_type_3.jsonl\"\n",
    "# ).exists()\n",
    "# # assert Path(bigclone_path / \"bigclonebenchmark_lg.csv\").exists()\n",
    "# # assert Path(bigclone_path / \"bigclonebenchmark_sm.csv\").exists()\n",
    "\n",
    "# bugfix_path = data_path / \"bug_fix\"\n",
    "# assert Path(bugfix_path / \"bug_fix_pairs.zip\").exists()\n",
    "# assert Path(bugfix_path / \"50-100/buggy\").exists()\n",
    "# assert Path(bugfix_path / \"50-100/fixed\").exists()\n",
    "\n",
    "# codesearchnet_path = data_path / \"codesearchnet\"\n",
    "# assert Path(codesearchnet_path / \"codesearchnet_java\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is a test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>भारत test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             code\n",
       "0  this is a test\n",
       "1       भारत test"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame([\"this is a test\", \"भारत test\"], columns=[\"code\"])\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _isASCII(mthd: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given method contains only ASCII characters. From https://stackoverflow.com/a/27084708/5768407.\n",
    "\n",
    "    :param mthd: the method to verify contains only ASCII characters\n",
    "    :returns: returns a boolean representing whether or not the given method contains only ASCII characters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mthd.encode(encoding=\"utf-8\").decode(\"ascii\")\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def remove_non_ascii(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove all methods that contain non-ascii characters from a given pandas dataframe, not in-place.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to be beautified\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a new dataframe without methods that contain non-ascii characters\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df = df[df.code.apply(_isASCII)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_ASCII_DF = pd.DataFrame([\"this is a test\"], columns=[\"code\"])\n",
    "df_non_ascii = remove_non_ascii(df_fake)\n",
    "\n",
    "assert (NON_ASCII_DF == df_non_ascii).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public void setPipelines(java.util.Collection&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code\n",
       "0  public void setPipelines(java.util.Collection<..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame(\n",
    "    [\n",
    "        \"\"\"public void setPipelines(java.util.Collection<Pipeline> pipelines) {\n",
    "        if (pipelines == null) {\n",
    "            this.pipelines = null;\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        this.pipelines = new com.amazonaws.internal.SdkInternalList<Pipeline>(pipelines);\n",
    "    }\n",
    "    \"\"\"\n",
    "    ],\n",
    "    columns=[\"code\"],\n",
    ")\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _beautify(mthd: str) -> str:\n",
    "    \"\"\"\n",
    "    Beautifies a given method using uncrustify with the sun.cfg style, i.e., Oracle's style.\n",
    "\n",
    "    :param mthd: the method to beautify\n",
    "    :returns: returns a beautified version of the given method\n",
    "    \"\"\"\n",
    "    # get path of icodegen\n",
    "    icodegen_path = Path(icodegen.__path__[0])\n",
    "\n",
    "    # create tmp file to store df contents for training tokenizer\n",
    "    tmp_path = Path(\"/tmp\")\n",
    "    tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(tmp_path / \"tmp.java\", \"w\") as f:\n",
    "        f.write(mthd)\n",
    "\n",
    "    try:\n",
    "        beaut_mthd = check_output(\n",
    "            [\n",
    "                icodegen_path / \"uncrustify\",\n",
    "                \"-c\",\n",
    "                icodegen_path / \"sun.cfg\",\n",
    "                \"-f\",\n",
    "                tmp_path / \"tmp.java\",\n",
    "            ]\n",
    "        ).decode(\"utf-8\")\n",
    "    except CalledProcessError as e:\n",
    "        # Exception thrown when the method is malformed, i.e, it is missing a curly brace\n",
    "        beaut_mthd = e.output.decode(\"utf-8\")\n",
    "\n",
    "    return beaut_mthd\n",
    "\n",
    "\n",
    "def beautify_code(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Beautify the methods in a pandas dataframe using uncrustify with the sun.cfg style, i.e., Oracle's style, not in-place.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to be beautified\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a modified dataframe with the methods beautified\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(_beautify)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAUT_MTHD = \"\"\"public void setPipelines(java.util.Collection<Pipeline> pipelines) {\n",
    "    if (pipelines == null) {\n",
    "\tthis.pipelines = null;\n",
    "\treturn;\n",
    "    }\n",
    "    this.pipelines = new com.amazonaws.internal.SdkInternalList<Pipeline>(\n",
    "\tpipelines);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "df_beaut = beautify_code(df_fake)\n",
    "\n",
    "assert BEAUT_MTHD == df_beaut.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# dicts of special tokens we are adding to the tokenizers so they do not get split\n",
    "\n",
    "extra_tokens = {\"<n>\": \"\\n\"}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html\n",
    "java_reserved_tokens = {\n",
    "    \"<abstract>\": \"abstract\",\n",
    "    \"<assert>\": \"assert\",\n",
    "    \"<boolean>\": \"boolean\",\n",
    "    \"<break>\": \"break\",\n",
    "    \"<byte>\": \"byte\",\n",
    "    \"<case>\": \"case\",\n",
    "    \"<catch>\": \"catch\",\n",
    "    \"<char>\": \"char\",\n",
    "    \"<class>\": \"class\",\n",
    "    \"<const>\": \"const\",\n",
    "    \"<continue>\": \"continue\",\n",
    "    \"<default>\": \"default\",\n",
    "    \"<do>\": \"do\",\n",
    "    \"<double>\": \"double\",\n",
    "    \"<else>\": \"else\",\n",
    "    \"<enum>\": \"enum\",\n",
    "    \"<extends>\": \"extends\",\n",
    "    \"<final>\": \"final\",\n",
    "    \"<finally>\": \"finally\",\n",
    "    \"<float>\": \"float\",\n",
    "    \"<for>\": \"for\",\n",
    "    \"<goto>\": \"goto\",\n",
    "    \"<if>\": \"if\",\n",
    "    \"<implements>\": \"implements\",\n",
    "    \"<import>\": \"import\",\n",
    "    \"<instanceof>\": \"instanceof\",\n",
    "    \"<int>\": \"int\",\n",
    "    \"<interface>\": \"interface\",\n",
    "    \"<long>\": \"long\",\n",
    "    \"<native>\": \"native\",\n",
    "    \"<new>\": \"new\",\n",
    "    \"<package>\": \"package\",\n",
    "    \"<private>\": \"private\",\n",
    "    \"<protected>\": \"protected\",\n",
    "    \"<public>\": \"public\",\n",
    "    \"<return>\": \"return\",\n",
    "    \"<short>\": \"short\",\n",
    "    \"<static>\": \"static\",\n",
    "    \"<strictfp>\": \"strictfp\",\n",
    "    \"<super>\": \"super\",\n",
    "    \"<switch>\": \"switch\",\n",
    "    \"<synchronized>\": \"synchronized\",\n",
    "    \"<this>\": \"this\",\n",
    "    \"<throw>\": \"throw\",\n",
    "    \"<throws>\": \"throws\",\n",
    "    \"<transient>\": \"transient\",\n",
    "    \"<try>\": \"try\",\n",
    "    \"<void>\": \"void\",\n",
    "    \"<volatile>\": \"volatile\",\n",
    "    \"<while>\": \"while\",\n",
    "}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/opsummary.html\n",
    "java_operator_tokens = {\n",
    "    \"<=>\": \"=\",\n",
    "    \"<+>\": \"+\",\n",
    "    \"<->\": \"-\",\n",
    "    \"<*>\": \"*\",\n",
    "    \"</>\": \"/\",\n",
    "    \"<%>\": \"%\",\n",
    "    \"<++>\": \"++\",\n",
    "    \"<-->\": \"--\",\n",
    "    \"<!>\": \"!\",\n",
    "    \"<==>\": \"==\",\n",
    "    \"<!=>\": \"!=\",\n",
    "    \"<greater>\": \">\",\n",
    "    \"<greater_equal>\": \">=\",\n",
    "    \"<lesser>\": \"<\",\n",
    "    \"<lesser_equal>\": \"<=\",\n",
    "    \"<&&>\": \"&&\",\n",
    "    \"<||>\": \"||\",\n",
    "    \"<?>\": \"?\",\n",
    "    \"<:>\": \":\",\n",
    "    \"<~>\": \"~\",\n",
    "    \"<double_lesser>\": \"<<\",\n",
    "    \"<double_greater>\": \">>\",\n",
    "    \"<triple_greater>\": \">>>\",\n",
    "    \"<&>\": \"&\",\n",
    "    \"<^>\": \"^\",\n",
    "    \"<|>\": \"|\",\n",
    "}\n",
    "\n",
    "java_structural_tokens = {\n",
    "    \"<{>\": \"{\",\n",
    "    \"<}>\": \"}\",\n",
    "    \"<[>\": \"[\",\n",
    "    \"<]>\": \"]\",\n",
    "    \"<lesser>\": \"<\",\n",
    "    \"<greater>\": \">\",\n",
    "    \"<(>\": \"(\",\n",
    "    \"<)>\": \")\",\n",
    "    \"<;>\": \";\",\n",
    "}\n",
    "\n",
    "java_extra_tokens = {\n",
    "    \"<@>\": \"@\",\n",
    "    \"<...>\": \"...\",\n",
    "    \"<null>\": \"null\",\n",
    "    \"<true>\": \"true\",\n",
    "    \"<false>\": \"false\",\n",
    "}\n",
    "\n",
    "# combination of all dictionaries\n",
    "java_special_tokens = {\n",
    "    **java_reserved_tokens,\n",
    "    **java_operator_tokens,\n",
    "    **java_structural_tokens,\n",
    "    **java_extra_tokens,\n",
    "    **extra_tokens,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&gt;&gt;&gt; &gt; + public ++ \\n\\n \\t \\t \\t\\t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  code\n",
       "0  >>> > + public ++ \\n\\n \\t \\t \\t\\t  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "df_fake = pd.DataFrame([\">>> > + public ++ \\n\\n \\t \\t \\t\\t  \"], columns=[\"code\"])\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _replace_toks(mthd: str, spec_toks: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Helper function for replacing all special tokens in a given method. This will replace longer special tokens first in order to not mistakenly breakup a special token that is part of a longer sequence. Adapted from https://stackoverflow.com/a/6117124/5768407 and https://stackoverflow.com/a/11753945/5768407\n",
    "\n",
    "    :param mthd: the method to have its special tokens replaced\n",
    "    :param spec_toks: a dictionary containing the special tokens to replace and the new tokens to replace them with\n",
    "    :returns: returns the method with its special tokens replaced\n",
    "    \"\"\"\n",
    "    # construct escaped versions of keys for running through regex\n",
    "    spec_toks = dict(\n",
    "        (re.escape(v), k)\n",
    "        for k, v in sorted(\n",
    "            java_special_tokens.items(), key=lambda x: len(x[1]), reverse=True\n",
    "        )\n",
    "    )\n",
    "    # construct regex pattern for finding all special tokens in a method\n",
    "    pattern = re.compile(\"|\".join(spec_toks.keys()))\n",
    "    # replace all special tokens in a method\n",
    "    mthd = pattern.sub(lambda m: spec_toks[re.escape(m.group(0))], mthd)\n",
    "\n",
    "    return mthd\n",
    "\n",
    "\n",
    "def replace_special_tokens(\n",
    "    df: pd.DataFrame, spec_toks: Dict[str, str], n: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace all the special tokens in a pandas dataframe.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to replace special tokens in\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :returns: returns a modified dataframe with the special tokens replaced\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(lambda mthd: _replace_toks(mthd, spec_toks))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACED_MTHD = \"<triple_greater> <greater> <+> <public> <++> <n><n> \\t \\t \\t\\t  \"\n",
    "df_replaced = replace_special_tokens(df_fake, java_special_tokens)\n",
    "\n",
    "assert REPLACED_MTHD == df_replaced.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "fake_data = \"<triple_greater> <greater> <+> <public> <++> <n><n>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _replace_spec_toks(mthd: str, spec_toks: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Performs the replacement of special tokens by the original ones\n",
    "    \"\"\"\n",
    "    spec_toks = spec_toks.copy()\n",
    "    # Add special tokenizer tokens -> deleted for code analysis\n",
    "    spec_toks['<bos>'] = \"\"\n",
    "    spec_toks['<eos>'] = \"\"\n",
    "    spec_toks['<pad>'] = \"\"\n",
    "    \n",
    "    spec_toks = dict(\n",
    "        (re.escape(k), v)\n",
    "        for k, v in sorted(\n",
    "            spec_toks.items(), key=lambda x: len(x[1]), reverse=True\n",
    "        )\n",
    "    )\n",
    "    # construct regex pattern for finding all special tokens in a method\n",
    "    pattern = re.compile(\"|\".join(spec_toks.keys()))\n",
    "    # replace all special tokens in a method\n",
    "    mthd = pattern.sub(lambda m: spec_toks[re.escape(m.group(0))], mthd)\n",
    "    \n",
    "    mthd = __replace_tokenizer_toks(mthd)\n",
    "    return mthd\n",
    "\n",
    "def replace_spec_toks_to_original(df: pd.DataFrame, spec_toks: Dict[str, str],\n",
    "                        n: Optional[int] = None) -> pd.DataFrame:\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(lambda m: _replace_spec_toks(m, spec_toks))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;triple_greater&gt; &lt;greater&gt; &lt;+&gt; &lt;public&gt; &lt;++&gt; &lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code\n",
       "0  <triple_greater> <greater> <+> <public> <++> <..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake = pd.DataFrame([\"<triple_greater> <greater> <+> <public> <++> <n><n> \\t \\t \\t\\t  \"], columns=[\"code\"])\n",
    "\n",
    "df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replaced = replace_spec_toks_to_original(df_fake, java_special_tokens)\n",
    "REPLACED_MTHD = \">>> > + public ++ \\n\\n \\t \\t \\t\\t  \"\n",
    "\n",
    "assert REPLACED_MTHD == df_replaced.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def __replace_tokenizer_toks(code_snippet: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to replace special tokens introduced by the tokenizer/model (bos, eos, pad)\n",
    "    :param code_snippet: String representing the code snippet\n",
    "    :return: String containing the clean string\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"|\".join([\"<pad>\", \"<sos>\", \"<eos>\"]))\n",
    "    \n",
    "    clean_snippet = pattern.sub(\"\", code_snippet)\n",
    "    return clean_snippet\n",
    "\n",
    "def replace_tokenizer_toks(df: pd.DataFrame, n: Optional[int]=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to replreace\n",
    "    :param df: Pandas DataFrame containing the collection of code snippets\n",
    "    :return: Clean DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(lambda snippet: __replace_tokenizer_toks(snippet))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public ahhhhhh () { }'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__replace_tokenizer_toks(\"<sos>public ahhhhhh () { }<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_snippet = \"<sos>public methodName () { }<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><eos>\"\n",
    "fake_df = pd.DataFrame([fake_snippet], columns=[\"code\"])\n",
    "REPLACED_SNIPPET = \"public methodName () { }\"\n",
    "replaced_df = replace_tokenizer_toks(fake_df)\n",
    "assert REPLACED_SNIPPET == replaced_df.code.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_tokenizer(\n",
    "    df: pd.DataFrame,\n",
    "    spec_toks: Dict[str, str],\n",
    "    max_length: int,\n",
    "    n: Optional[int] = None,\n",
    "    vocab_sz: Optional[int] = 10_000,\n",
    "    min_freq: Optional[int] = 2,\n",
    "    output: Optional[Path] = None,\n",
    ") -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Train a ByteLevel BPE tokenizer on a given pandas dataframe. Code adapted from https://github.com/huggingface/tokenizers/tree/master/bindings/python.\n",
    "\n",
    "    :param df: the pandas dataframe containing each method to have the tokenizer train on\n",
    "    :param spec_toks: dict of special tokens to add to the tokenizers so they do not get split\n",
    "    :param n: the number of methods to evaluate. If none, the entire dataframe will be used\n",
    "    :param vocab_sz: the maximum vocabulary size of the trained tokenizer. Defaulted was selected from: Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code\n",
    "    :param min_freq: the minimum frequency a token has to occur to be considered\n",
    "    :returns: returns a trained ByteLevel BPE tokenizer\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "\n",
    "    # create tmp file to store df contents for training tokenizer\n",
    "    tmp_path = Path(\"/tmp\")\n",
    "    tmp_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(tmp_path / \"tmp_tokenize.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(df.code.values[:n]))\n",
    "\n",
    "    # initialize a tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # customize pre-tokenization and decoding\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "    # train tokenizer with data in tmp file\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_sz,\n",
    "        min_frequency=min_freq,\n",
    "        special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\"] + list(spec_toks.keys()),\n",
    "    )\n",
    "    tokenizer.train(trainer, [str(tmp_path / \"tmp_tokenize.txt\")])\n",
    "    tokenizer.enable_padding(length=max_length, pad_token=\"<pad>\")\n",
    "    tokenizer.enable_truncation(max_length)\n",
    "\n",
    "    # save tokenizer if output path given\n",
    "    if output is not None:\n",
    "        tokenizer.save(output, pretty=True)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_SPEC = [\n",
    "    \"<triple_greater>\",\n",
    "    \"Ġ\",\n",
    "    \"<greater>\",\n",
    "    \"Ġ\",\n",
    "    \"<+>\",\n",
    "    \"Ġ\",\n",
    "    \"<public>\",\n",
    "    \"Ġ\",\n",
    "    \"<++>\",\n",
    "    \"Ġ\",\n",
    "    \"<n>\",\n",
    "    \"<n>\",\n",
    "    \"<pad>\",\n",
    "    \"<pad>\",\n",
    "    \"<pad>\",\n",
    "    \"<pad>\",\n",
    "]\n",
    "max_length = 16\n",
    "tokenizer = train_tokenizer(df_fake, java_special_tokens, max_length)\n",
    "encoded = tokenizer.encode(fake_data)\n",
    "\n",
    "assert TOKENIZED_SPEC == encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _split_input_target(mthd):\n",
    "    input_text = mthd[:-1]\n",
    "    target_text = mthd[1:]\n",
    "\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "def convert_df_to_tfds(\n",
    "    df: pd.DataFrame, tokenizer: Tokenizer, max_length: int, batch_size: int\n",
    "):\n",
    "    tokenized_mthds = []\n",
    "    for i in range(0, len(df.code.values), batch_size):\n",
    "        batch = df.code.values[i : i + batch_size]\n",
    "        batch = [f\"<sos>{x}\" for x in batch]\n",
    "        for x in tokenizer.encode_batch(batch):\n",
    "            tokenized_mthds.append(x.ids)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(tokenized_mthds)\n",
    "    ds = ds.map(_split_input_target).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataset = convert_df_to_tfds(df_fake, tokenizer, max_length, batch_size)\n",
    "\n",
    "for _, shape in dataset:\n",
    "    assert (batch_size, max_length - 1) == shape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "TYPES = 3\n",
    "def _process_bigclone(path):\n",
    "    for i in range(1, TYPES + 1):\n",
    "        df = pd.read_json(path / f\"bigclone-type-{i}.jsonl\", orient=\"records\", lines=True).rename(columns={\"function_1\": \"code\"})\n",
    "        df = remove_non_ascii(df)\n",
    "        \n",
    "        control_df = df[\"code\"].to_frame()\n",
    "        control_df = replace_special_tokens(control_df, java_special_tokens)\n",
    "        \n",
    "        treatment_df = df[\"function_2\"].to_frame().rename(columns={\"function_2\": \"code\"})\n",
    "        treatment_df = replace_special_tokens(treatment_df, java_special_tokens)\n",
    "        \n",
    "        big_clone_df = pd.DataFrame(\n",
    "            zip(control_df.code.values, treatment_df.code.values),\n",
    "            columns=[\"function_1\", \"function_2\"]\n",
    "        )\n",
    "        big_clone_df.to_json(path / f\"bigclone-type-{i}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "def _process_bug_fix(path):\n",
    "    buggy_paths = sorted((path / \"50-100\").glob(\"buggy/*.java\"))\n",
    "    fixed_paths = sorted((path / \"50-100\").glob(\"fixed/*.java\"))\n",
    "    bugs = []\n",
    "    fixes = []\n",
    "    for bug_p, fix_p in zip(buggy_paths, fixed_paths):\n",
    "        with open(bug_p, \"r\") as f:\n",
    "            bugs.append(f.read())\n",
    "\n",
    "        with open(fix_p, \"r\") as f:\n",
    "            fixes.append(f.read())\n",
    "\n",
    "    df_fix_bug = pd.DataFrame(zip(bugs, fixes), columns=[\"code\", \"fixes\"])\n",
    "    df_fix_bug = remove_non_ascii(df_fix_bug)\n",
    "    df_buggy = pd.DataFrame(df_fix_bug.code.values, columns=[\"code\"])\n",
    "#     df_buggy = remove_non_ascii(df_buggy)\n",
    "    df_buggy = replace_special_tokens(df_buggy, java_special_tokens)\n",
    "\n",
    "    df_fixed = pd.DataFrame(df_fix_bug.fixes.values, columns=[\"code\"])\n",
    "#     df_fixed = remove_non_ascii(df_fixed)\n",
    "    df_fixed = replace_special_tokens(df_fixed, java_special_tokens)\n",
    "\n",
    "    # Saving to jsonl because csv formatting is causing issues with quoting\n",
    "    df_buggy.to_json(path / \"buggy.jsonl\", orient=\"records\", lines=True)\n",
    "    df_fixed.to_json(path / \"fixed.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "def _jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat(\n",
    "        [\n",
    "            pd.read_json(f, orient=\"records\", compression=\"gzip\", lines=True)[columns]\n",
    "            for f in file_list\n",
    "        ],\n",
    "        sort=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def _process_codesearchnet(path):\n",
    "    \"\"\"\n",
    "    Grabs the different data splits and converts them into dataframes.\n",
    "    Expects format from Code Search Net Challenge.\n",
    "    \"\"\"\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted((path / \"java\" / \"final\" / \"jsonl\" / split).glob(\"**/*.gz\"))\n",
    "        df = _jsonl_list_to_dataframe(files, [\"code\"])\n",
    "        df = remove_non_ascii(df)\n",
    "        df = replace_special_tokens(df, java_special_tokens)\n",
    "        # Saving to jsonl because csv formatting is causing issues with quoting\n",
    "        if split == \"train\":\n",
    "            # 10% selected to match the Big Code != Big Vocab paper.\n",
    "            df_trn, df_bpe = train_test_split(df, test_size=0.1)\n",
    "            df_trn.to_json(path / f\"{split}.jsonl\", orient=\"records\", lines=True)\n",
    "            df_bpe.to_json(path / \"bpe.jsonl\", orient=\"records\", lines=True)\n",
    "        else:\n",
    "            df.to_json(path / f\"{split}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "def _process_comment_testbed(path, out_path):\n",
    "    df_tst = pd.read_json(path / \"test.jsonl\", orient=\"records\", lines=True)\n",
    "    df_replaced = replace_spec_toks_to_original(df_tst, java_special_tokens)\n",
    "    df_no_cmts = transform_df(df_replaced, java_comment_remover)\n",
    "    df_no_cmts = replace_special_tokens(df_no_cmts, java_special_tokens)\n",
    "    \n",
    "    # remove duplicates\n",
    "    df_combined = pd.concat(\n",
    "        [\n",
    "            df_tst.rename(columns={\"code\": \"commented\"}), df_no_cmts.rename(columns={\"code\": \"uncommented\"})\n",
    "        ], axis=1\n",
    "    )\n",
    "    non_dups = df_combined[df_combined.apply(lambda x: x[\"commented\"] != x[\"uncommented\"], axis = 1)]\n",
    "    \n",
    "    df_cmtd = pd.DataFrame(non_dups.commented.values, columns=[\"code\"])\n",
    "    df_uncmtd = pd.DataFrame(non_dups.uncommented.values, columns=[\"code\"])\n",
    "    \n",
    "#     .code.update(non_dups.commented)\n",
    "#     df_no_cmts.code.update(non_dups.uncommented)\n",
    "#     df_no_cmts.code.values = non_dups.uncommented.values\n",
    "    \n",
    "    df_cmtd.to_json(out_path/\"commented_code.jsonl\", orient=\"records\", lines=True)\n",
    "    df_uncmtd.to_json(out_path/\"uncommented_code.jsonl\", orient=\"records\", lines=True)\n",
    "    \n",
    "\n",
    "    \n",
    "def process_data(path):\n",
    "    \"\"\"Function for processing data related to the library.\"\"\"\n",
    "    # Process Bug Fix Pairs data\n",
    "    bugfix_path = path / \"bug_fix\"\n",
    "    _process_bug_fix(bugfix_path)\n",
    "\n",
    "    # Process CodeSearchNet Challenge data\n",
    "    codesearchnet_path = path / \"codesearchnet\"\n",
    "    _process_codesearchnet(codesearchnet_path / \"codesearchnet_java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/jovyan/work/dvc-icodegen/testbed/ts-bigclone-types\")\n",
    "_process_bigclone(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/jovyan/work/dvc-icodegen/datax/searchnet/codesearchnet-java\")\n",
    "out_path = Path(\"/home/jovyan/work/dvc-icodegen/testbed/ts-comments\")\n",
    "_process_comment_testbed(path, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26716, 6664, 6664)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tst = pd.read_json(path / \"test.jsonl\", orient=\"records\", lines=True)\n",
    "df_cmtd = pd.read_json(out_path / \"commented_code.jsonl\", orient=\"records\", lines=True)\n",
    "df_uncmtd = pd.read_json(out_path / \"uncommented_code.jsonl\", orient=\"records\", lines=True)\n",
    "len(df_tst), len(df_cmtd), len(df_uncmtd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bugfix_path = path / \"bug_fix\"\n",
    "bugfix_path = Path(\"/home/jovyan/work/dvc-icodegen/bug_fix\")\n",
    "_process_bug_fix(bugfix_path)\n",
    "df_buggy = pd.read_json(bugfix_path / \"buggy.jsonl\", orient=\"records\", lines=True)\n",
    "df_fixed = pd.read_json(bugfix_path / \"fixed.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64722, 64722)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_buggy), len(df_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65455, 65455)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_buggy), len(df_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def process_java_df(df: pd.DataFrame, n: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs preprocessing (rm non ascii and replacement of special chars.)\n",
    "    :param df: Pandas Dataframe containing code (in 'code' column)\n",
    "    :param n: Number of records to process\n",
    "    :return: Processed pandas dataframe \n",
    "    \"\"\"\n",
    "    clean_df = remove_non_ascii(df, n)\n",
    "    clean_df = replace_special_tokens(clean_df, java_special_tokens)\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/tmp/data/codesearchnet\")\n",
    "df_trn = pd.read_json(\n",
    "    data_path / \"codesearchnet_java\" / \"train.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")[:50]\n",
    "ds_trn = convert_df_to_tfds(df_trn, tokenizer, max_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(1, 15), dtype=int32) Tensor(\"args_1:0\", shape=(1, 15), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# def huggingface_format(x, y):\n",
    "#     print(x, y)\n",
    "#     return {\n",
    "#         \"input_ids\": x,\n",
    "#         \"labels\": y\n",
    "#     }\n",
    "#     return x, y\n",
    "\n",
    "# ds_trn = ds_trn.map(huggingface_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  50, 102,  96,  96,  94,  99,  98,  96,  83,  98,\n",
      "         96, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  50, 102,  96,  96,  94,  99,  98,  96,  83,  98,  96,\n",
      "        102,  98]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  96,  91, 100,  37, 102,   5, 102,  96,  97,  98,  96,\n",
      "         83,  20]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  96,  91, 100,  37, 102,   5, 102,  96,  97,  98,  96,  83,\n",
      "         20, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  35, 102, 102,  98,  98,  83,  10, 102,  95,  96,  84, 102,\n",
      "         79,  91]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 35, 102, 102,  98,  98,  83,  10, 102,  95,  96,  84, 102,  79,\n",
      "         91, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  40, 102,  29, 102,  95,  97,  97,  98,  83,  20,\n",
      "        102,  95]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  40, 102,  29, 102,  95,  97,  97,  98,  83,  20, 102,\n",
      "         95,  97]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  97, 102,  95,  99,  95,  99,  97,  95,  96,  96,\n",
      "         96,  97]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  97, 102,  95,  99,  95,  99,  97,  95,  96,  96,  96,\n",
      "         97,  95]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  95,  99, 102,  96,  98,  83,  20, 102, 102,  96,\n",
      "         97, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  95,  99, 102,  96,  98,  83,  20, 102, 102,  96,  97,\n",
      "        102,  20]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  50, 102,  97,  97,  95,  83,  84,  91, 102, 102,\n",
      "        102, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  50, 102,  97,  97,  95,  83,  84,  91, 102, 102, 102,\n",
      "        102, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  99,  95,  96, 102,  96,  96,  96,  94,  97,  83,\n",
      "         99,  96]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  99,  95,  96, 102,  96,  96,  96,  94,  97,  83,  99,\n",
      "         96,  97]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  96,  91, 102, 102, 102, 102,  37, 102,  29, 102,  15,\n",
      "         83,  84]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  96,  91, 102, 102, 102, 102,  37, 102,  29, 102,  15,  83,\n",
      "         84, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  36, 102,  66, 102,  19, 102,  99,  98,  99,  64, 102,  50,\n",
      "        102,  99]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 36, 102,  66, 102,  19, 102,  99,  98,  99,  64, 102,  50, 102,\n",
      "         99,  98]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  40, 102,  96, 102,  98,  83,  96, 102, 102,  96,\n",
      "        102,  84]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  40, 102,  96, 102,  98,  83,  96, 102, 102,  96, 102,\n",
      "         84, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  98,  96,  91, 102, 102,  37, 102,  40, 102,  96,  97,\n",
      "         97,  99]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  98,  96,  91, 102, 102,  37, 102,  40, 102,  96,  97,  97,\n",
      "         99,  96]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  98,  96,  97,  96,  66,  97,  66,  70,  64,  81,\n",
      "         82,  64]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  98,  96,  97,  96,  66,  97,  66,  70,  64,  81,  82,\n",
      "         64, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  35, 102,   5, 102,  95,  95,  96,  96,  83, 102,  95,  84,\n",
      "        102,  79]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 35, 102,   5, 102,  95,  95,  96,  96,  83, 102,  95,  84, 102,\n",
      "         79,  91]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  99,  98,  99, 102,  96,  97,  96,  83,  20, 102,\n",
      "         99,  98]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  99,  98,  99, 102,  96,  97,  96,  83,  20, 102,  99,\n",
      "         98,  99]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  99,  96,  97,  96,  66,  99,  94,  97,  96,  95,\n",
      "         64, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  99,  96,  97,  96,  66,  99,  94,  97,  96,  95,  64,\n",
      "        102,  99]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  50, 102,  98,  29,  96,  97,  96,  83,  96, 102,\n",
      "        102,  99]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  50, 102,  98,  29,  96,  97,  96,  83,  96, 102, 102,\n",
      "         99,  95]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  96,  91, 102, 102, 102, 102,  37, 102,  44, 102,  99,\n",
      "         94,  99]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  96,  91, 102, 102, 102, 102,  37, 102,  44, 102,  99,  94,\n",
      "         99,  94]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  40, 102,  50, 102,  98,  83,  97,  97,  95,  96,\n",
      "         66,  70]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  40, 102,  50, 102,  98,  83,  97,  97,  95,  96,  66,\n",
      "         70,  64]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  96, 102,  97,  96,  83,  97,  99,  23,  96, 102,\n",
      "         95,  97]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  96, 102,  97,  96,  83,  97,  99,  23,  96, 102,  95,\n",
      "         97,  99]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  36, 102,  40, 102,  98,  96, 102,  83,  84, 102,  79,  91,\n",
      "        102, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 36, 102,  40, 102,  98,  96, 102,  83,  84, 102,  79,  91, 102,\n",
      "        102, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  99,  98,  98,  96,  83,  99,  95,  95,  84,  91, 102,\n",
      "        102,  37]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  99,  98,  98,  96,  83,  99,  95,  95,  84,  91, 102, 102,\n",
      "         37, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  50, 102,  95,  96,  99,  94,  98,  98, 102,  83,\n",
      "         84, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  50, 102,  95,  96,  99,  94,  98,  98, 102,  83,  84,\n",
      "        102,  47]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,   7,  81,  82, 102,  83,  84, 102,  79,  91, 102,\n",
      "        102, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,   7,  81,  82, 102,  83,  84, 102,  79,  91, 102, 102,\n",
      "        102, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  40, 102,  96,  97,  97,  96, 102,  94,  99,  96,\n",
      "         97,  97]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  40, 102,  96,  97,  97,  96, 102,  94,  99,  96,  97,\n",
      "         97,  96]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  44, 102,  50, 102,  98,  83,  29, 102,  96,  96,\n",
      "         97,  98]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  44, 102,  50, 102,  98,  83,  29, 102,  96,  96,  97,\n",
      "         98,  95]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  98,  95,  91, 102, 102, 102, 102,  37, 102,  40, 102,\n",
      "         66,  64]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  98,  95,  91, 102, 102, 102, 102,  37, 102,  40, 102,  66,\n",
      "         64, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  36, 102,  98,  97, 102,  95,  98,  83,  96,  94,  97,  97,\n",
      "        102,  96]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 36, 102,  98,  97, 102,  95,  98,  83,  96,  94,  97,  97, 102,\n",
      "         96,  94]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  40, 102,  66, 102,  19, 102,  95,  96,  98,  94,\n",
      "         97,  64]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  40, 102,  66, 102,  19, 102,  95,  96,  98,  94,  97,\n",
      "         64, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  96,  91, 100,  37, 102,   5, 102,  95,  96,  83,  94,\n",
      "         95, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  96,  91, 100,  37, 102,   5, 102,  95,  96,  83,  94,  95,\n",
      "        102,  84]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  40, 102,  96,  98,  98, 102,  95,  83,  96,  81,\n",
      "         82, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  40, 102,  96,  98,  98, 102,  95,  83,  96,  81,  82,\n",
      "        102, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  40, 102,  99, 102,  83,  99, 102,  98,  94,  99,\n",
      "        102,  95]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  40, 102,  99, 102,  83,  99, 102,  98,  94,  99, 102,\n",
      "         95,  96]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  96,  91, 102, 102,  37, 102,  50, 102,  96,  98,  83,\n",
      "         84, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  96,  91, 102, 102,  37, 102,  50, 102,  96,  98,  83,  84,\n",
      "        102,  79]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,   5, 102,  97,  96,  83,  96, 102,  96,  84, 102,\n",
      "         47, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,   5, 102,  97,  96,  83,  96, 102,  96,  84, 102,  47,\n",
      "        102,  95]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  20, 102,   5, 102,  96,  95,  97,  98,  83,  96,\n",
      "        102,  98]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  20, 102,   5, 102,  96,  95,  97,  98,  83,  96, 102,\n",
      "         98,  98]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  95,  96,  96, 102,  83,  91, 102, 102, 102, 102,\n",
      "        102, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  95,  96,  96, 102,  83,  91, 102, 102, 102, 102, 102,\n",
      "        102, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  95,  96,  94,  97,  99,  96,  99,  97, 102,  96,\n",
      "         99,  97]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  95,  96,  94,  97,  99,  96,  99,  97, 102,  96,  99,\n",
      "         97,  83]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  96,  91, 102, 102, 102,  37, 102,  20, 102,  50, 102,\n",
      "         98,  83]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  96,  91, 102, 102, 102,  37, 102,  20, 102,  50, 102,  98,\n",
      "         83,  84]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  96, 102,  83,  96,  95, 102, 102,  96, 102,  84,\n",
      "        102,  79]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  96, 102,  83,  96,  95, 102, 102,  96, 102,  84, 102,\n",
      "         79,  91]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  35, 102,  96,  66,  64, 102,  96,  83,  84, 102,  79,  91,\n",
      "         91, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 35, 102,  96,  66,  64, 102,  96,  83,  84, 102,  79,  91,  91,\n",
      "        102, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  96,  91, 100,  37, 102,  50, 102,  94,  94,  83,  94,\n",
      "         94, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  96,  91, 100,  37, 102,  50, 102,  94,  94,  83,  94,  94,\n",
      "        102,  95]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  40, 102,  96,  95,  99,  99,  66,  96,  64, 102,\n",
      "         83,  20]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  40, 102,  96,  95,  99,  99,  66,  96,  64, 102,  83,\n",
      "         20, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  96,  91, 102, 102, 102, 102,  37, 102,  50, 102,  99,\n",
      "         98,  97]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  96,  91, 102, 102, 102, 102,  37, 102,  50, 102,  99,  98,\n",
      "         97,  83]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  96,  91, 102, 102, 102, 102,  37, 102,   5, 102,  96,\n",
      "         97,  83]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  96,  91, 102, 102, 102, 102,  37, 102,   5, 102,  96,  97,\n",
      "         83,  84]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  96,  95,  99,  99,  66,  96,  66,  99,  96,  95,\n",
      "         96,  98]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  96,  95,  99,  99,  66,  96,  66,  99,  96,  95,  96,\n",
      "         98,  95]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  86,  99,  98,  98,  96,  83,  99,  95,  95,  84,  91, 100,\n",
      "         37, 102]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 86,  99,  98,  98,  96,  83,  99,  95,  95,  84,  91, 100,  37,\n",
      "        102,  40]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  40, 102,  96,  96,  98, 102,  83,  20, 102,  96,\n",
      "         96,  98]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  40, 102,  96,  96,  98, 102,  83,  20, 102,  96,  96,\n",
      "         98, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  96,  66,  95,  96,  94,  98,  97,  96,  95,  96,\n",
      "         99,  64]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  96,  66,  95,  96,  94,  98,  97,  96,  95,  96,  99,\n",
      "         64, 102]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  66,  96,  64, 102,  94,  83,  84, 102,  79,  91,\n",
      "        100, 100]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  66,  96,  64, 102,  94,  83,  84, 102,  79,  91, 100,\n",
      "        100,  66]], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[  1,  37, 102,  96, 102,  98,  29,  83,  94,  97,  96,  97, 102,\n",
      "         98,  96]], dtype=int32)>, 'labels': <tf.Tensor: shape=(1, 15), dtype=int32, numpy=\n",
      "array([[ 37, 102,  96, 102,  98,  29,  83,  94,  97,  96,  97, 102,  98,\n",
      "         96,  97]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "# for e in ds_trn:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at sshleifer/tiny-gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2TokenizerFast, TFGPT2LMHeadModel\n",
    "# from icodegen.model.core import _loss\n",
    "\n",
    "# trnsfr_tokenizer = GPT2TokenizerFast.from_pretrained(\"sshleifer/tiny-gpt2\")\n",
    "# tokenizer = trnsfr_tokenizer.backend_tokenizer\n",
    "# trnsfr = TFGPT2LMHeadModel.from_pretrained(\"sshleifer/tiny-gpt2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:757 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:498 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:598 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/utils.py:78 filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable: %s.\" %\n\n    ValueError: No gradients provided for any variable: ['tfgp_t2lm_head_model_2/transformer/wte/weight:0', 'tfgp_t2lm_head_model_2/transformer/wpe/embeddings:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/ln_1/gamma:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/ln_1/beta:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/attn/c_attn/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/attn/c_attn/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/attn/c_proj/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/attn/c_proj/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/ln_2/gamma:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/ln_2/beta:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/mlp/c_fc/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/mlp/c_fc/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/mlp/c_proj/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/mlp/c_proj/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/ln_1/gamma:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/ln_1/beta:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/attn/c_attn/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/attn/c_attn/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/attn/c_proj/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/attn/c_proj/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/ln_2/gamma:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/ln_2/beta:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/mlp/c_fc/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/mlp/c_fc/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/mlp/c_proj/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/mlp/c_proj/bias:0', 'tfgp_t2lm_head_model_2/transformer/ln_f/gamma:0', 'tfgp_t2lm_head_model_2/transformer/ln_f/beta:0'].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-555a4643afce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrnsfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrnsfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = trnsfr.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mds_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:757 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:498 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:598 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/utils.py:78 filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable: %s.\" %\n\n    ValueError: No gradients provided for any variable: ['tfgp_t2lm_head_model_2/transformer/wte/weight:0', 'tfgp_t2lm_head_model_2/transformer/wpe/embeddings:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/ln_1/gamma:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/ln_1/beta:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/attn/c_attn/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/attn/c_attn/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/attn/c_proj/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/attn/c_proj/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/ln_2/gamma:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/ln_2/beta:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/mlp/c_fc/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/mlp/c_fc/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/mlp/c_proj/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._0/mlp/c_proj/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/ln_1/gamma:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/ln_1/beta:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/attn/c_attn/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/attn/c_attn/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/attn/c_proj/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/attn/c_proj/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/ln_2/gamma:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/ln_2/beta:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/mlp/c_fc/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/mlp/c_fc/bias:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/mlp/c_proj/weight:0', 'tfgp_t2lm_head_model_2/transformer/h_._1/mlp/c_proj/bias:0', 'tfgp_t2lm_head_model_2/transformer/ln_f/gamma:0', 'tfgp_t2lm_head_model_2/transformer/ln_f/beta:0'].\n"
     ]
    }
   ],
   "source": [
    "# trnsfr.compile(optimizer=\"adam\", loss=trnsfr.compute_loss)\n",
    "# history = trnsfr.fit(\n",
    "#     ds_trn, epochs=1, batch_size=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out since depends on downloaded data\n",
    "# process_data(data_path)\n",
    "\n",
    "# assert Path(bugfix_path / \"buggy.jsonl\").exists()\n",
    "# assert Path(bugfix_path / \"fixed.jsonl\").exists()\n",
    "\n",
    "# assert Path(\n",
    "#     codesearchnet_path / \"codesearchnet_java\" / \"train.jsonl\"\n",
    "# ).exists()\n",
    "# assert Path(\n",
    "#     codesearchnet_path / \"codesearchnet_java\" / \"bpe.jsonl\"\n",
    "# ).exists()\n",
    "# assert Path(\n",
    "#     codesearchnet_path / \"codesearchnet_java\" / \"valid.jsonl\"\n",
    "# ).exists()\n",
    "# assert Path(\n",
    "#     codesearchnet_path / \"codesearchnet_java\" / \"test.jsonl\"\n",
    "# ).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUGGY_MTHD = \"\"\"\\\n",
    "# private void success(io.netty.channel.Channel channel) {\n",
    "#     org.mycat.netty.mysql.MySQLHandshakeHandler.logger.debug(\"success info return form MySQLHandshakeHandler\");\n",
    "#     io.netty.buffer.ByteBuf out = channel.alloc().buffer();\n",
    "#     org.mycat.netty.mysql.OK ok = new org.mycat.netty.mysql.OK();\n",
    "#     ok.sequenceId = 2;\n",
    "#     ok.setStatusFlag(Flags.SERVER_STATUS_AUTOCOMMIT);\n",
    "#     out.writeBytes(ok.toPacket());\n",
    "#     channel.writeAndFlush(out);\n",
    "# }\"\"\"\n",
    "# FIXED_MTHD = \"\"\"\\\n",
    "# private void success(io.netty.channel.Channel channel) {\n",
    "#     org.mycat.netty.mysql.MySQLHandshakeHandler.logger.info(\"success info return form MySQLHandshakeHandler\");\n",
    "#     io.netty.buffer.ByteBuf out = channel.alloc().buffer();\n",
    "#     org.mycat.netty.mysql.OK ok = new org.mycat.netty.mysql.OK();\n",
    "#     ok.sequenceId = 2;\n",
    "#     ok.setStatusFlag(Flags.SERVER_STATUS_AUTOCOMMIT);\n",
    "#     out.writeBytes(ok.toPacket());\n",
    "#     channel.writeAndFlush(out);\n",
    "# }\"\"\"\n",
    "# df = pd.read_json(bugfix_path / \"bug_fix_pairs.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "# assert BUGGY_MTHD == df.buggy.values[0] and FIXED_MTHD == df.fixed.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.core.ipynb.\n",
      "Converted 01_data.transforms.ipynb.\n",
      "Converted 02_model.core.ipynb.\n",
      "Converted 04_evaluation.core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
